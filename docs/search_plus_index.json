{"./":{"url":"./","title":"Part Zero. Introduction ","keywords":"","body":"Introduction 常用终端命令 $ gitbook install ./ $ gitbook build ./ .\\algorithm-notes.github.io\\docs $ gitbook serve Katex 语法 Inline math: {% math %}\\int_{-\\infty}^\\infty g(x) dx{% endmath %} Block math: {% math %} \\int_{-\\infty}^\\infty g(x) dx {% endmath %} 正文引用添加方式 > [!note|iconVisibility:hidden] > NOTE > [!tip|iconVisibility:hidden] > TIPS > [!warning|iconVisibility:hidden] > SUPPLEMENT > [!danger|iconVisibility:hidden] > CORE CONTENT "},"【开发手册】Python.html":{"url":"【开发手册】Python.html","title":"Part One. Python 开发手册","keywords":"","body":"关于 Python 的使用 字符串补全固定长度 >>> '789'.ljust(5, '0') '78900' >>> '789'.rjust(5, '0') '00789' >>> '789'.zfill(5) '00789' 对列表的快速等分 import math def chunk1(l,chunk_size): return list(map(lambda x:l[x*chunk_size:(x+1)*chunk_size],range(0,math.ceil(len(l)/chunk_size))) def chunk2(l,chunk_size): return [l[x:x + chunk_size] for x in range(0,len(l),chunk_size)] l=[0,1,2,3,4,5,6,7,8,9] chunk1(l,3) [out]: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] chunk2(l,4) [out]: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9]] os.walk os.walk() 方法用于通过在目录树中游走输出在目录中的文件名，向上或者向下 import os for root, dirs, files in os.walk(\".\", topdown=False): for name in files: print(os.path.join(root, name)) for name in dirs: print(os.path.join(root, name)) 文件系统操作—— pathlib 将 pathlib.Path 对象转换为字符串 with open(pic_name, 'wb') as image: image.write(download.content) image_path = Path(pic_name).resolve() return image_path 当我打印 image_path 时，我得到了图像的完整路径 常用属性 p.name #返回目录或文件名，str p.stem #返回目录或文件名，不带后缀，str p.suffix #返回后缀，str，如'.jpg' p.parent #返回父级目录，WindowsPath p.parents #返回父级目录列表，[WindowsPath] p.parts #拆分路径，(str)元组，如('E:\\\\', '20200907', 'ImgFloder', '0_right.jpg') p.drive #返回目录或文件所在盘符，str，如'E:' p.root #返回根目录，str，如'//' p.anchor #自动判断韩慧root或drive 常用方法 p.open() #通常用open(p,mode)方式 p.mkdir() #创建目录 p.cwd() #返回当前目录的路径对象 p.stat() #返回目录或文件信息 p.home() #返回当前用户的根目录 p.with_name() #更改最后一级路劲名 p.with_suffix() #更改后缀 p.is_absolute() #是否是绝对路径 p.is_reserved() #是否是预留路径 控制浮点数输出 if __name__ == '__main__': a = 3.1415926 print(round(a, 4)) # print(f\"a = {round(a, 4)}\") print(\"%.2f\" % a) print(\"{:.3f}\".format(a)) map map() 会根据提供的函数对指定序列做映射 map(function, iterable, ...) 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表 Python 2.x 返回列表 Python 3.x 返回迭代器 >>> def square(x) : # 计算平方数 ... return x ** 2 ... >>> map(square, [1,2,3,4,5]) # 计算列表各个元素的平方 # 返回迭代器 >>> list(map(square, [1,2,3,4,5])) # 使用 list() 转换为列表 [1, 4, 9, 16, 25] >>> list(map(lambda x: x ** 2, [1, 2, 3, 4, 5])) # 使用 lambda 匿名函数 [1, 4, 9, 16, 25] multiprocessing multiprocessing.Pool 可以提供指定数量的进程供用户调用，当有新的请求提交到 pool 中时，如果池还没满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到 规定最大，那么该请求就会等待，直到迟中有进程结束，才会创建新的进程来执行它 Pool 类用于需要执行的目标很多、而手动限制进程数量又太繁琐时，如果目标少且不用控制进程数量可以用 Process 类 class multiprocessing.pool.Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]]) from multiprocessing import Pool def f(x): return x*x if __name__ == '__main__': with Pool(5) as p: print(p.map(f, [1, 2, 3])) apply apply(func[,args[, kwds]]) 使用参数 args 和关键字参数 kwds 调用 func。它会阻塞，直到结果准备就绪。鉴于此块，更适合并行执行工作。此外，func 仅在池中的一个工作程序中执行 from multiprocessing import Pool import time def test(p): print(p) time.sleep(3) if __name__==\"__main__\": pool = Pool(processes=10) for i in range(500): ''' ('\\n' ' （1）遍历500个可迭代对象，往进程池放一个子进程\\n' ' （2）执行这个子进程，等子进程执行完毕，再往进程池放一个子进程，再执行。（同时只执行一个子进程）\\n' ' for循环执行完毕，再执行print函数。\\n' ' ') ''' pool.apply(test, args=(i,)) #维持执行的进程总数为10，当一个进程执行完后启动一个新进程. print('test') pool.close() pool.join() for循环内执行的步骤顺序：往进程池中添加一个子进程，执行子进程，等待执行完毕再添加一个子进程……等500个子进程都执行完了，再执行print。（从结果来看，并没有多进程并发） apply_async apply_async(func[, args[, kwds[, callback[, error_callback]]]]) 异步进程池（非阻塞），如果指定了回调，则它应该是可调用的，它接受单个参数。当结果变为就绪时，将对其应用回调，除非调用失败，在这种情况下将应用error_callback 如果指定了 error_callback，那么它也应该是一个可调用的，它接受一个参数。如果目标函数失败，则使用异常实例调用 error_callback。回调应立即完成，否则处理结果的线程将被阻止 from multiprocessing import Pool import time def test(p): print(p) time.sleep(3) if __name__==\"__main__\": pool = Pool(processes=2) for i in range(500): ''' （1）循环遍历，将500个子进程添加到进程池（相对父进程会阻塞）\\n' （2）每次执行2个子进程，等一个子进程执行完后，立马启动新的子进程。（相对父进程不阻塞）\\n' ''' pool.apply_async(test, args=(i,)) #维持执行的进程总数为 2，当一个进程执行完后启动一个新进程. print('test') pool.close() pool.join() 调用 join 之前，先调用 close 或者 terminate 方法，否则会出错。执行完 close 后不会有新的进程加入到 pool，join 函数等待所有子进程结束 map map(func, iterable[, chunksize]) map() 内置函数的并行等价物（尽管它只支持一个可迭代的参数）。它会阻塞，直到结果准备就绪。此方法将 iterable 内的每一个对象作为单独的任务提交给进程池。可以通过将 chunksize 设置为正整数来指定这些块的（近似）大小 from multiprocessing import Pool def test(i): print(i) if __name__ == \"__main__\": lists = [1, 2, 3] pool = Pool(processes=2) #定义最大的进程数 pool.map(test, lists) #p必须是一个可迭代变量。 pool.close() pool.join() map_async map_async(func, iterable[, chunksize[, callback[, error_callback]]]) from multiprocessing import Pool import time def test(p): print(p) time.sleep(3) if __name__==\"__main__\": pool = Pool(processes=2) pool.map_async(test, range(500)) print('test') pool.close() pool.join() imap imap(func, iterable[, chunksize]) 返回迭代器，next() 调用返回的迭代器的方法得到结果 close 防止任何更多的任务被提交到池中，一旦完成所有任务，工作进程将退出 terminate 立即停止工作进程而不完成未完成的工作。当池对象被来及收集时，terminate() 将立即调用 join 等待工作进程退出，必须在 join() 之前调用 close() 或 terminate() from multiprocessing import Pool import time def f(x): return x*x if __name__ == '__main__': with Pool(processes=4) as pool: # start 4 worker processes result = pool.apply_async(f, (10,)) # evaluate \"f(10)\" asynchronously in a single process print(result.get(timeout=1)) # prints \"100\" unless your computer is *very* slow print(pool.map(f, range(10))) # prints \"[0, 1, 4,..., 81]\" it = pool.imap(f, range(10)) print(next(it)) # prints \"0\" print(next(it)) # prints \"1\" print(it.next(timeout=1)) # prints \"4\" unless your computer is very slow result = pool.apply_async(time.sleep, (10,)) print(result.get(timeout=1)) # raises multiprocessing.TimeoutError ''' 100 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 0 1 4 Traceback (most recent call last): File \"C:/Users/BruceWong/Desktop/develop/multiprocessingpool.py\", line 19, in print(next(res)) TypeError: 'MapResult' object is not an iterator Process finished with exit code 1 ''' exec() exec() 算是 python 的一个奇技淫巧，给定一个字符串代码，然后使用 exec() 来执行字符串代码 def exec_code(): LOC = \"\"\" def factorial(num): fact=1 for i in range(1,num+1): fact = fact*i return fact print(factorial(5)) \"\"\" exec(LOC) exec_code() tqdm tqdm 的导入是 from tqdm import tqdm，而不是直接 import tqdm 使用 tqdm 的三种方法： tqdm(list)，如，for i in tqdm(range(1000)) trange，其实就是 tqdm(range()) 的简写 在 for 循环外部初始化 tqdm，可以打印其他信息，如 bar = tqdm([\"a\", \"b\", \"c\", \"d\"]) for char in pbar: pbar.set_description(\"Processing %s\" % char) zip 函数 zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回有这些元组组成的对象 >>> a = [1,2,3] >>> b = [4,5,6] >>> c = [4,5,6,7,8] >>> zipped = zip(a,b) # 返回一个对象 >>> zipped >>> list(zipped) # list() 转换为列表 [(1, 4), (2, 5), (3, 6)] >>> list(zip(a,c)) # 元素个数与最短的列表一致 [(1, 4), (2, 5), (3, 6)] >>> a1, a2 = zip(*zip(a,b)) # 与 zip 相反，zip(*) 可理解为解压，返回二维矩阵式 >>> list(a1) [1, 2, 3] >>> list(a2) [4, 5, 6] >>> filter() 函数 Python内置的 filter() 函数能够从可迭代对象（如字典、列表）中筛选某些元素，并生成一个新的迭代器 基本语法如下所示 filter(function, iterable) 返回一个可迭代的 filter 对象，可以使用 list() 将其转化为列表，这个列表包含过滤器对象中返回的所有的项 filter()所提供的过滤方法，通常比用列表解析更有效，特别是当我们处理更大的数据集时。例如，列表解析会生成一个新列表，这会增加该处理的运行时间。同时，当列表解析执行完毕它的表达式后，内存中会有两个列表。但是，filter() 只生成一个简单的对象，该对象包含对原始列表的引用、提供的函数以及原始列表中位置的索引，这样操作占用的内存更少 下面介绍 filter() 的不同用法： ① 在 filter 中使用特殊函数 filter()的第一个参数是一个函数，用它来决定第二个参数所引用的可迭代对象中的每一项的去留。此函数被调用后，当返回 False 时，第二个参数中的可迭代对象里面相应的值就会被删除。针对这个函数，可以是一个普通函数，也可以使用 lambda 函数，特别是当表达式不那么复杂的时候，如 filter(lambda item: item[] expression, iterable) ② 在 filter 中使用 None 我们也可以将 None 作为 filter() 的第一个参数，让迭代器过滤掉 Python 中布尔值是 False 的对象，比如长度为 0 的对象（如空列表或空字符串）或在数字上等于 0 的对象 >>> aquarium_tanks = [11, False, 18, 21, \"\", 12, 34, 0, [], {}] >>> filtered_tanks = filter(None, aquarium_tanks) >>> print(list(filtered_tanks)) [11, 18, 21, 12, 34] ③ 将 filter 用于字典 举个例子，假设我们有水族馆里每种生物的一个列表以及每种生物的不同细节，用下面的列表显示此数据 aquarium_creatures = [ {\"name\": \"sammy\", \"species\": \"shark\", \"tank number\": \"11\", \"type\": \"fish\"}, {\"name\": \"ashley\", \"species\": \"crab\", \"tank number\": \"25\", \"type\": \"shellfish\"}, {\"name\": \"jo\", \"species\": \"guppy\", \"tank number\": \"18\", \"type\": \"fish\"}, {\"name\": \"jackie\", \"species\": \"lobster\", \"tank number\": \"21\", \"type\": \"shellfish\"}, {\"name\": \"charlie\", \"species\": \"clownfish\", \"tank number\": \"12\", \"type\": \"fish\"}, {\"name\": \"olly\", \"species\": \"green turtle\", \"tank number\": \"34\", \"type\": \"turtle\"} ] 我们构建一个函数来按照用户输入过滤数据 def filter_set(aquarium_creatures, search_string): def iterator_func(x): for v in x.values(): if search_string in v: return True return False return filter(iterator_func, aquarium_creatures) filter_set 这个函数根据用户输入的 search_string，在字典每个键的值中查找是否存在 search_string 的项，然后通过 filter() 进行过滤。例如，我们查找键值对中是否带有字符 2 的项 >>> filtered_records = filter_set(aquarium_creatures, \"2\") >>> print(list(filtered_records)) [{'name': 'ashley', 'species': 'crab', 'tank number': '25', 'type': 'shellfish'}, {'name': 'jackie', 'species': 'lobster', 'tank number': '21', 'type': 'shellfish'}, {'name': 'charlie', 'species': 'clownfish', 'tank number': '12', 'type': 'fish'}] Pandas 专栏 读写 csv 使用 pandas 读 csv 文件 直接使用 read_csv() 方法 import pandas as pd df = pd.read_csv('../data_pro/audito_whole.csv') print(df) 有些时候会遇到下面的错误 ParserError: Error tokenizing data. C error: Expected 1 fields in line 29, saw 2 csv 文件默认的是以逗号为分隔符，但是中文中逗号的使用率很高，爬取中文数据时就容易造成混淆，所以使用 pandas 写入 csv 时可以设置参数 sep=’\\t’ ，即以 tab 为分隔符写入。毕竟 tab 在中文习惯里用的很少。那这样在后面读取 csv 进行数据处理时，一定记得加上一个参数 delimiter df=pd.read_csv('path',delimiter=\"\\t\") 将字典转换为 DateFrame d = {'one': [1, 2, 3], 'two': [1, 2, 3]} df = pd.DataFrame(d,index=['a', 'b', 'c']) print(df) 输出 one two a 1 1 b 2 2 c 3 3 将 DateFrame 转换为字典 >>> df = pd.DataFrame({'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]}) >>> df a b 0 red 0.500 1 yellow 0.250 2 blue 0.125 dict - 默认值：列名是键，值是索引的字典：数据对 >>> df.to_dict('dict') {'a': {0: 'red', 1: 'yellow', 2: 'blue'}, 'b': {0: 0.5, 1: 0.25, 2: 0.125}} list - 键是列名，值是列数据列表 >>> df.to_dict('list') {'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]} 系列 - orient = 'series’ 和 orient = 'list’ 的唯一区别：这里的 value 是 series 数据类型，而 list 是列表类型 >>> df.to_dict('series') {'a': 0 red 1 yellow 2 blue Name: a, dtype: object, 'b': 0 0.500 1 0.250 2 0.125 Name: b, dtype: float64} split - 将列 / 数据 / 索引拆分为键，值分别为列名，数据值分别按行和索引标签 >>> df.to_dict('split') {'columns': ['a', 'b'], 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]], 'index': [0, 1, 2]} 记录 - 每一行都成为一个字典，其中键是列名，值是单元格中的数据 >>> df.to_dict('records') > [{'a': 'red', 'b': 0.5}, {'a': 'yellow', 'b': 0.25}, {'a': 'blue', 'b': 0.125}] index - 类似于 records，但是一个字典字典，其中键作为索引标签（而不是列表） >>> df.to_dict('index') {0: {'a': 'red', 'b': 0.5}, 1: {'a': 'yellow', 'b': 0.25}, 2: {'a': 'blue', 'b': 0.125}} 读写 json 文件 函数 作用 json.dumps 对数据进行编码，将 python 中的字典转换为 JSON 字符串 json.loads 对数据进行解码，将 JSON 字符串转换为 python 中的字典 json.dump 将 dict 数据写入 json 文件中 json.load 打开 json 文件，并把字符串转换为 python 的 dict 数据 将数据转换成字符串 >>> import json >>> test_dict = {'one':1, 'two':{2.1:['a', 'b']} >>> print(test_dict) {'one':1, 'two':{2.1:['a', 'b']} >>> print(type(test_dict)) # dumps 将数据转换成字符串 >>> json_str = json.dumps(test_dict) >>> print(json_str) {\"one\":1, \"two\":{2.1:[\"a\", \"b\"]} >>> print(type(json_str)) 写入数据 将字典数据用 dumps() 方法编码成 JSON 字符串，然后再写入 json 文件中 with open(\"../config/format_json.json\", 'w') as write_f: write_f.write(json.dumps(load_dict, indent=4, ensure_ascii=False)) 直接用 dump() 方法将字典数据写入 json 文件中 with open(\"../config/format_json.json\", 'w') as write_f: json.dump(load_dict, write_f, indent=4, ensure_ascii=False) 读取数据 with open(\"res.json\", 'r', encoding='utf-8') as fw: injson = json.load(fw) print(injson) 读写 txt 覆盖写 file_name = 'a.txt' with open(f, 'w') as file: file.write(\"xxxx\") 追加写 file_name = 'a.txt' with open(f, 'a') as file: file.write(\"xxxx\") 如果要保存的数据很简单就是一个 numpy array 的话，可以通过下面代码实现存取 np.savetxt(\"xxx.txt\",arr) arr= np.loadtxt(\"xxx.txt\") # The read data type is numpy array time.time() Python time time() 返回当前时间的时间戳（1970纪元后经过的浮点秒数） >>> import time >>> print \"time.time(): %f \" % time.time() 1234892919.655932 >>> print time.localtime( time.time() ) time.struct_time(tm_year=2022, tm_mon=3, tm_mday=12, tm_hour=12, tm_min=56, tm_sec=7, tm_wday=5, tm_yday=71, tm_isdst=0) >>> print time.asctime( time.localtime(time.time()) ) Tue Feb 17 10:48:39 2009 我们一般会选用第二种方式来生成一些文件的名称，例如在 Tensorboard >>> h = time.strftime('%y_%m_%d_%H_%M_%S', time.localtime(time.time())) 22_03_12_13_01_07 eval() eval() 函数用来执行一个字符串表达式，并返回表达式的值 >>>x = 7 >>> eval( '3 * x' ) 21 >>> eval('pow(2,2)') 4 >>> eval('2 + 2') 4 >>> n=81 >>> eval(\"n + 4\") 85 argparse argparse--- 命令行选项、参数和子命令解析器 argparse 模块是 Python 内置的一个用于命令项选项与参数解析的模块 代码示例 import argparse # 创建一个解析器 parser = argparse.ArgumentParser(description='test') # 添加参数 parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.') parser.add_argument('--seed', type=int, default=72, help='Random seed.') parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.') # 解析参数 args = parser.parse_args() print(args.sparse) print(args.seed) print(args.epochs) 创建 ArgumentParser() 对象 使用 argparse 的第一步是创建一个 ArgumentParser 对象，ArgumentParser 对象包含将命令行解析成 Python 数据类型所需的全部信息 大多数对 ArgumentParser 构造方法的调用都会使用 description= 关键字参数，这个参数简要描述这个程度做什么以及怎么做。在帮助消息中，这个描述会显示在命令行用法字符串和各种参数的帮助消息之间 关于 action='store_true' 的说明：action 命令行遇到参数时的动作，默认值是 store，action='store_true' 只要运行时该变量有传参就将该变量设为True 关于 -- 和 - 的区别：args 分为可选参数（用 -- 指定）和必选参数（用 -指定），如果在定义 xxx 参数时，没有用 -- 指定，那么该参数需要在命令行内手动指定，此时即使通过 default 设置默认参数也还是会报错 调用 add_argument() 方法添加参数 给一个 ArgumentParser 添加程序参数信息是通过调用 add_argument() 方法完成的。通常，这些调用指定 ArgumentParser 如何获取命令行字符串并将其转换为对象。这些信息在 parse_args() 调用时被存储和使用 使用 parse_args() 解析添加的参数 ArgumentParser 通过 parse_args() 方法解析参数。它将检查命令行，把每个参数转换为适当的类型然后调用相应的操作。在脚本中，通常 parse_args() 会被不带参数调用，而 ArgumentParser 将自动从 sys.argv 中确定命令行参数。 Jupyter 执行 shell 命令 在使用 jupyter 时，可以直接在命令之前放一个 “ ! ”，就能执行 shell 命令，完全不用切换来切换去，就能在 IPython 里执行任何命令行，甚至可以将值传递给shell，像下面这样： .npy 和 .npz 文件 很多时候我们用大模型抽特征都会使用 npy 文件来存取特征，这里简单的给出 npy 文件存取的方式 import numpy as np np.save('xxx.npy', arr=data) data = np.load('xxx.npy') 这里我们区分一下 .npy 和 .npz 文件， npy 是 numpy 专用的二进制格式文件，使用时，数组会以未压缩的原始二进制格式保存在扩展名为.npy 的文件中 使用 np.savez() 函数可以将多个数组保存到同一个文件中。函数的第一个参数是文件名，其后的参数都是需要保存的数组。函数输出的是一个扩展名为 .npz 的压缩文件，它包含多个与保存数组对应的 npy 文件，文件名对应数组名。读取 .npz 文件时同样使用 np.load() 函数，返回的是一个类似于字典的对象，可以通过数组名作为关键字对多个数组进行访问 print(xx, flush = True) print() 函数会把内容放到内存中，内存中的内容并不一定能够及时刷新显示到屏幕中。 使用 flush=True 之后，会在 print 结束之后，不管你有没有达到条件，立即将内存中的东西显示到屏幕上，清空缓存。 具体的使用场景不是太明确 print 的一些显示技巧 通过 \\r 和 end=\"\" 的结合，我们可以单行刷新输出，而不是每个输出一个行，这个在 print loss 时非常友好 import time for i in range(10): print(f\"\\r count number: {i}\", end=\"\") time.sleep(1) 关于 Numpy 的使用 array 升维 import numpy as np arr = np.array([[1, 2, 3], [4, 5, 6]]) print(arr.shape) # (2, 3) arr_1 = arr[:, :, None] print(arr_1.shape) # (2, 3, 1) arr_2 = arr[:, None, :] print(arr_2.shape) # (2, 1, 3) array 拼接 np.concatenate 函数，axis 为 0 时，按行拼接，为 1 时按列拼接。用法如下： np.concatenate([a,b],axis=0) np.vstack 与 np.hstack 函数，np.vstack 按行拼接，np.hstack 按列拼接。用法如下： np.vstack([a,b]) np.hstack([a,b]) np.r_ 与 np.c_ 函数，np.r_ 按行拼接，np.c_按列拼接。用法如下： np.r_[a,b] np.c_[a,b] np.tile 关于 Pytorch 使用 打印完整的向量 numpy 打印前添加 np.set_printoptions(threshold=np.inf) print(x) torch 打印前添加 torch.set_printoptions(profile=\"full\") print(x) # prints the whole tensor torch.set_printoptions(profile=\"default\") # reset print(x) # prints the truncated tensor timm timm 是一个巨大的 PyTorch 代码集合，整合了常用的 models、layers、utilities、optimizers、schedulers、data-loaders/augmentations 和 reference training/validation scripts 查看所有模型 model_list = timm.list_models() print(model_list) 基本像 ResNet、ViT、Swin这些模型都有 查看具有预训练参数的模型 model_pretrain_list = timm.list_models(pretrained=True) print(model_pretrain_list) 检索特定模型 model_resnet = timm.list_models('*resnet*') print(model_resnet) 创建模型 x = torch.randn((1, 3, 256, 512)) modle_mobilenetv2 = timm.create_model('mobilenetv2_100', pretrained=True) out = modle_mobilenetv2(x) print(out.shape) # torch.Size([1, 1000]) 改变输出类别数 x = torch.randn((1, 3, 256, 512)) modle_mobilenetv2 = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=100) out = modle_mobilenetv2(x) print(out.shape) # torch.Size([1, 100]) 改变输入通道数 x = torch.randn((1, 10, 256, 512)) modle_mobilenetv2 = timm.create_model('mobilenetv2_100', pretrained=True, in_chans=10) out = modle_mobilenetv2(x) print(out.shape) # torch.Size([1, 1000]) 提取特征 x = torch.randn((1, 3, 256, 512)) modle_mobilenetv2 = timm.create_model('mobilenetv2_100', pretrained=True, features_only=True) out = modle_mobilenetv2(x) for o in out: print(o.shape) # torch.Size([1, 16, 128, 256]) # torch.Size([1, 24, 64, 128]) # torch.Size([1, 32, 32, 64]) # torch.Size([1, 96, 16, 32]) # torch.Size([1, 320, 8, 16]) nn.Parasmeter() 其函数原形如下所示，torch.nn.parameter.Parameter(data=None, requires_grad=True)，将一个不可训练的 tensor 转换成可以训练的 parameter 类型，并将这个 parameter 绑定到 module 里面。如我们常见的 transformer 中的 cls token 就是这么定义的 self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dims)) torch.nn.Module.add_module add_module 的用法很简单，就是在当前的 module 里面添加一个子 module，如下所示 import torch.nn as nn class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.add_module(\"conv\", nn.Conv2d(10, 20, 4)) #self.conv = nn.Conv2d(10, 20, 4) is the same as above model = Model() print(model.conv)#Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1) 一般不会在模型 init 去使用 add_module，add_module 提供了一种在 init 外定义子模块的方法，常用来替换模块，例如模型在 init 时声明了一个 conv 层 self.conv1=nn.Conv2d(...)，那么在模型初始化之后，可以使用 model.add_module(\"conv1\", nn.Conv3d(...)) 来替换掉模型中的 conv 层 如果使用了 cuda，并且多卡，需要将模型放回 cpu 后再进行修改 scatter 和 scatter_ 先说 scatter 和 scatter 的区别，scatter不会直接修改原来的 tensor，而 scatter 是直接在原 tensor 上进行修改 scatter(dim, index, src) 的作用是将 src 中的数据根绝 index 中的所有按照 dim 指定的方向进行填充 # 填充规则(dim=0) self[ index[i][j] ] [j] = src[i][j] >>> x = torch.rand((2, 3)) >>> x tensor([[0.8442, 0.2668, 0.9861], [0.6431, 0.0584, 0.1365]]) >>> torch.zeros(3, 3).scatter(0, torch.LongTensor([[0, 1, 2], [0, 1, 1]]), x) tensor([[0.6431, 0.0000, 0.0000], [0.0000, 0.0584, 0.1365], [0.0000, 0.0000, 0.9861]]) # 填充规则(dim=1) self [i] [ index[i][j] ] = src[i][j] 注意，这里的 index 必须是 torch.LongTensor 类型 scatter 一般常用于 one-hot 编码，如 >>> class_num = 10 >>> batch_size = 4 >>> label = torch.LongTensor(batch_size, 1).random_() % class_num tensor([[6], [0], [3], [2]]) torch.zeros(batch_size, class_num).scatter_(1, label, 1) tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]) nn.Conv1d nn.Conv1d 的函数原型如下所示 torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) 官方给出的数学描述如下 需要注意的是，in_channels 和 out_channels 的关系，nn.Conv1d(1, 20, 5) 表示输入 1 通道，20 个卷积核，核大小为 5 例如，下面这个代码中，inchannels = 16，out_channels = 33，所以输出 output.shape[1] = 33，根据卷积公式 $N = (W − F + 2P )/S+1$，$W$ 为输入图片大小，$F$ 为 kernel 大小，步长 $S$，padding $P$，输出 feature map $N$， 那么 $(50 - 3 + 2 * 0) / 2 + 1 = 24$ >>> m = nn.Conv1d(16, 33, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) >>> output.shape torch.Size([20, 33, 24]) 一般我们在使用 Conv1D 代替 Linear 处理一个 3-d features (e.g. shape: [bs, frames, hidden_dim]) 时都会有一个 transpose 操作，目的是使得操作过程类似于 Linear，如下所示 class Conv1D(nn.Module): def __init__(self, in_dim, out_dim, kernel_size=1, stride=1, padding=0, bias=True): super(Conv1D, self).__init__() self.conv1d = nn.Conv1d(in_channels=in_dim, out_channels=out_dim, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias) def forward(self, x): # suppose all the input with shape (batch_size, seq_len, dim) x = x.transpose(1, 2) # (batch_size, dim, seq_len) x = self.conv1d(x) return x.transpose(1, 2) # (batch_size, seq_len, dim) 关于 Con1D 输入和输出特征形状的计算，官方也给出了一个公式 下面我们介绍一下 groups 这个参数，在 DepthWiseSeparableConv 时一般都会用到这个参数 下面是一个 Xception DepthWiseSeparableConv1d 的示例 深度可分离卷积可理解为：深度卷积 + 逐点卷积，深度卷积只处理长宽方向的空间信息；逐点卷积只处理跨通道方向的信息。能大大减少参数量，提高计算效率 深度卷积： 是一个卷积核只处理一个通道，即每个卷积核只处理自己对应的通道。输入特征图有多少个通道就有多少个卷积核。将每个卷积核处理后的特征图堆叠在一起。输入和输出特征图的通道数相同 由于只处理长宽方向的信息会导致丢失跨通道信息，为了将跨通道的信息补充回来，需要进行逐点卷积。逐点卷积： 是使用1x1卷积对跨通道维度处理，有多少个1x1卷积核就会生成多少个特征图 关于 Inception 和 Xception 的详细内容可以参考 【神经网络】(15) Xception 代码复现，网络解析，附Tensorflow完整代码 同时，depthwise（3 x 3 处理单个通道）和 pointwise（1 x 1 处理所有通道）的前后顺序不影响结果，且在这两层之间不加入激活函数效果更好 class DepthWiseSeparableConv1d(nn.Module): \"\"\" Xception: Deep Learning with Depthwise Separable Convolutions (https://arxiv.org/pdf/1610.02357.pdf) \"\"\" def __init__(self,in_channels,out_channels,kernel_size,stride=1,bias=True): super().__init__() self.depth_wise = nn.Conv1d(in_channels,in_channels,kernel_size,stride,padding=kernel_size//2,groups=in_channels,bias=bias) self.point_wise = nn.Conv1d(in_channels,out_channels,kernel_size=1,bias=bias) nn.init.kaiming_normal_(self.depth_wise.weight) nn.init.kaiming_normal_(self.point_wise.weight) if bias: nn.init.constant_(self.depth_wise.bias, 0.0) nn.init.constant_(self.point_wise.bias, 0.0) def forward(self,x): x = x.transpose(1, 2) x1 = self.depth_wise(x) x2 = self.point_wise(x1) return x2.transpose(1, 2) contiguous PyTorch 中提供了 is_contiguous 和 contiguous 两个方法，分别用于判定 Tensor 是否是 contiguous 的，以及保证 Tensor 是 contiguous 的 is_contiguous 直观的解释是 Tensor 底层一维数组元素的存储顺序与 Tensor 按行优先一维展开的元素顺序是否一致 Tensor 多维数组底层实现是使用一块连续内存的1维数组（行优先顺序存储），Tensor 在元信息里保存了多维数组的形状，在访问元素时，通过多维度索引转化成 1维数组相对于数组起始位置的偏移量即可找到对应的数据。某些Tensor操作（如transpose、permute、narrow、expand）与原 Tensor 是共享内存中的数据，不会改变底层数组的存储，但原来在语义上相邻、内存里也相邻的元素在执行这样的操作后，在语义上相邻，但在内存不相邻，即不连续了（is not contiguous） 为什么需要 contiguous 操作？ 一方面，torch.view 等方法操作需要连续的 Tensor transpose、permute 操作虽然没有修改底层一维数组，但是新建了一份 Tensor 元信息，并在新的元信息中重新指定了 stride。torch.view 方法约定了不修改数组本身，只是使用新的形状查看数据。如果我们在 transpose、permute 操作后执行 view，Pytorch 会抛出错误 如， >>>t = torch.arange(12).reshape(3,4) >>>t tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) >>>t.stride() (4, 1) >>>t2 = t.transpose(0,1) >>>t2 tensor([[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]]) >>>t2.stride() (1, 4) >>>t.data_ptr() == t2.data_ptr() # 底层数据是同一个一维数组 True >>>t.is_contiguous(),t2.is_contiguous() # t连续，t2不连续 (True, False) t2 与 t 引用同一份底层数据 a（如下），两者仅是 stride、shape 不同 a = [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] 如果执行 t2.view(-1) ，期望返回以下数据 b，会发生错误 b = [ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11] 原因是在 a 的基础上使用一个新的 stride 无法直接得到 b ，需要先使用 t2 的 stride (1, 4) 转换到 t2 的结构，再基于 t2 的结构使用 stride (1,) 转换为形状为 (12,)的 b，但这不是 view 工作的方式，view 仅在底层数组上使用指定的形状进行变形，所以发生了错误 另一方面，处于性能的考虑，连续的 Tensor，语义上相邻的元素，在内存中也是连续的，访问相邻元素是矩阵运算中经常用到的操作，语义和内存顺序的一致性是缓存友好的，在内存中连续的数据可以（但不一定）被高速缓存预取，以提升CPU获取操作数据的速度。transpose、permute 后使用 contiguous 方法则会重新开辟一块内存空间保证数据是在逻辑顺序和内存中是一致的，连续内存布局减少了 CPU 对内存的请求次数，相当于空间换时间 nn.Embedding nn.Embedding 定义为一个 lookup table，存储固定大小的 dictionary，输入 indices 来获取指定序号的 word embedding 向量 >>> # an Embedding module containing 10 tensors of size 3 >>> embedding = nn.Embedding(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) >>> embedding(input) tensor([[[ 0.2908, -0.6090, -0.1527], [ 0.0762, 0.5815, 2.4304], [-0.1224, 0.2295, 0.8497], [ 0.6891, -0.1460, -0.2573]], [[-0.1224, 0.2295, 0.8497], [-0.9805, -0.3313, -1.5403], [ 0.0762, 0.5815, 2.4304], [ 0.1637, -0.1038, -1.4319]]], grad_fn=) nn.Embedding(10, 3) 构造了一个 vocab size = 10, 每个 vocabulary 用 3-d 向量表示的 table，embedding.weight 的每一行可以看做是一个词的向量表示 >>> embedding.weight Parameter containing: Parameter containing: tensor([[-0.6726, -0.5384, -0.1513], [ 0.2908, -0.6090, -0.1527], [ 0.0762, 0.5815, 2.4304], [-0.9805, -0.3313, -1.5403], [-0.1224, 0.2295, 0.8497], [ 0.6891, -0.1460, -0.2573], [-0.8668, 0.8097, -0.9668], [ 0.6575, -1.3596, 0.4976], [-1.2128, 0.2471, -2.3069], [ 0.1637, -0.1038, -1.4319]], requires_grad=True) 输入为 torch.LongTensor([[1,2,4,5],[4,3,2,9]])，相当于 input 有两个句子，每个句子由 4 个单词构成 于是我们可以看到，输入的第一句的第一单词的索引为 1，对应的词向量即是 [ 0.2908, -0.6090, -0.1527] torch.triu torch.triu(input, diagonal=0, out=None) → Tensor 返回矩阵上三角 如果 diagonal 为空，输出为原矩阵主对角线与主对角线以上的元素 如果 diagonal 为 n，输出为原矩阵主对角线与主对角线以上除去 n 行的元素 如果 diagonal 为 -n，输出为原矩阵主对角线与主对角线以上与主对角线下方 n 行对角线的元素； 举个例子，一看就懂 diagonal 的用法 >>> x = torch.randn((3, 3)) >>> x tensor([[-1.2126, 0.4228, -1.2225], [ 1.4905, -0.4956, 1.2763], [ 0.7451, -0.2384, -0.1493]]) >>> torch.triu(x) tensor([[-1.2126, 0.4228, -1.2225], [ 0.0000, -0.4956, 1.2763], [ 0.0000, 0.0000, -0.1493]]) >>> torch.triu(x, 1) tensor([[ 0.0000, 0.4228, -1.2225], [ 0.0000, 0.0000, 1.2763], [ 0.0000, 0.0000, 0.0000]]) >>> torch.triu(x, -1) tensor([[-1.2126, 0.4228, -1.2225], [ 1.4905, -0.4956, 1.2763], [ 0.0000, -0.2384, -0.1493]]) torch.where torch.where(condition, x, y) 可以看做满足 condition 时，取 x 中的元素，不满足时取 y 中元素 DataLoader、Dataset 和 Sampler 我们先看一下，DataLoader.__next__ 的源码（取 num_work= 0） class DataLoader(object): ... def __next__(self): if self.num_workers == 0: indices = next(self.sample_iter) # Sampler batch = self.collate_fn([self.dataset[i] for i in indices]) # Dataset if self.pin_memory: batch = _utils.pin_memory.pin_memory_batch(batch) return batch 假设我们输入的数据是一组图像，每一张图像对应着一个 index，读取数据即找到 index 对应的图像即可，而选择 index 的方法有多种，至于怎么选取这就是 Sampler 的工作，返回的 indices 即是将要生成 batch 的图像的 index，接着就是去 DataSet 取数据，在下面的 if 语句简单理解就是条件成立时 Pytorch 会通过一系列操纵把数据拷贝到 GPU，目的就是加速运算。三者的关系大致如下图所示 Sampler 为了更细致地理解 Sampler 的原理，我们看下 DataLoader 的传入参数。初始化参数里有两种 sampler：sampler 和 batch_sampler，前者的作用是生成一系列的 index，而 batch_sampler 则是将 sampler 生成的 indices 打包分组，得到一个又一个 batch 的 index class DataLoader(object): def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) 例如下面示例中，BatchSampler 将 SequentialSampler 生成的 index 按照指定的batch size 分组 >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False)) [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] Pytorch 中已经实现的 Sampler 有如下几种：SequentialSampler、RandomSampler、WeightedSampler 和 SubsetRandomSampler 关于 sampler 和 batch_sampler 的使用，有如下总结， 如果你自定义了 batch_sampler，那么这些参数都必须使用默认值：batch_size，shuffle，sampler，drop_last 如果你自定义了 sampler，那么 shuffle 需要设置为 False 如果 sampler 和 batch_sampler 都为 None，,那么 batch_sampler 使用 Pytorch 已经实现好的 BatchSampler，此时 sampler 分两种情况： 若 shuffle=True，则 sampler=RandomSampler(dataset) 若 shuffle=False，则 sampler=SequentialSampler(dataset) 至于如何自定义 Sampler 和 BatchSampler，可以从源码的一个父类—— Sampler 开始，所有的从采样器其实都继承自这个父类 class Sampler(object): r\"\"\"Base class for all Samplers. Every Sampler subclass has to provide an :meth:`__iter__` method, providing a way to iterate over indices of dataset elements, and a :meth:`__len__` method that returns the length of the returned iterators. .. note:: The :meth:`__len__` method isn't strictly required by :class:`~torch.utils.data.DataLoader`, but is expected in any calculation involving the length of a :class:`~torch.utils.data.DataLoader`. \"\"\" def __init__(self, data_source): pass def __iter__(self): raise NotImplementedError def __len__(self): return len(self.data_source) 我们需要做的是就是定义好 __iter__(self) 函数，不妥需要注意的是该函数返回的值需要是可迭代的，例如，SequentialSampler返回的是iter(range(len(self.data_source))) 至于 BatchSampler，它与其他 Sampler 的主要区别是它需要将 Sampler 作为参数进行打包，进而每次迭代返回以 batch size 为大小的 index 列表。也就是说在后面的读取数据过程中使用的都是 batch sampler Dataset Dataset 的定义方式如下，这三个方法是最基本的，其中 ___getitem__ 最为重要，它规定了如何读取数据。但是它又不同于一般的方法，因为它是 python built-in 方法，其主要作用是能让该类可以像 list 一样通过索引值对数据进行访问。假如定义好了一个 dataset，那么你可以直接通过 dataset[0] 来访问第一个数据 class Dataset(object): def __init__(self): ... def __getitem__(self, index): return ... def __len__(self): return ... 于是，如果想对 __getitem__ 方法进行调试，可以写一个 for 循环遍历 dataset 来进行调试了，而不用构建 dataloader 等一大堆东西。另外，其实通过前面的Dataloader的 __next__ 函数可以看到DataLoader对数据的读取其实就是用了for循环来遍历数据。仔细观察可以发现，在生成 batch 时调用了一个 self.collate_fn 方法，其中 indices 表示每一个iteration sampler返回的 indices，self.dataset[i] 就是对第 i 个数据进行读取操作，一般来说 self.dataset[i]=(img, label)。不难看出 collate_fn的作用就是将一个 batch 的数据进行合并。默认的 collate_fn 是将 img 和 label 分别合并成 imgs 和 labels，所以如果 Dataset 的 __getitem__ 方法只是返回 img, label，那么就可以使用默认的 collate_fn 方法，但是如果每次读取的数据有 img, box, label 等等，那么就需要自定义 collate_fn 来将对应的数据合并成一个 batch 数据，方便后续的训练步骤 tensor 拷贝 torch 为了提高速度，向量或是矩阵的赋值是指向同一内存的，如果需要深拷贝一个 tensor 的话可以用 clone() >>> a = torch.tensor(1.0, requires_grad=True) >>> b = a.clone() >>> print(c) tensor(2., grad_fn=) grad_fn= 表示 clone 后的返回值是个中间变量，支持梯度回传。clone 充当中间变量，会将梯度传给源张量叠加，但本身不保存其梯度，grad=None >>> a = torch.tensor(1.0, requires_grad=True) >>> a_ = a.clone() >>> y = a**2 >>> z = a ** 2+a_ * 3 >>> y.backward() >>> print(a.grad) tensor(2.) >>> z.backward() >>> print(a_.grad) None >>> print(a.grad) tensor(7.) # 2*2+3=7 使用 torch.clone() 获得的新 tensor 和原来的数据不再共享内存，但仍保留在计算图中，clone 操作在不共享内存数据的同时支持梯度传递与叠加，所以常用在神经网络中某单元需要重复使用的场景下 tensor 维度操作 torch.cat 在指定的维度 dim 上对序列 seq 进行连接操作 torch.stack 先扩张再拼接 tensor.squeeze 除去输入张量中数值为 1 的维度，并返回新的张量。如果输入张量的形状为$(A1BC1D)$，那么输出张量的形状为 $(ABCD)$ 当通过 dim 参数指定维度时，维度压缩操作只会在指定的维度上进行。如果输入向量的形状为 $(A1B)$，squeeze(input, 0) 会保持张量的维度不变，只有在执行 squeeze(input, 1) 时，输入张量的形状会被压缩至 $(A*B)$ 为何只去掉 1 呢？多维张量本质上就是一个变换，如果维度是 1 ，那么，1 仅仅起到扩充维度的作用，而没有其他用途，因而，在进行降维操作时，为了加快计算，是可以去掉这些 1 的维度 tensor.unsqueeze 即与 squeeze 相反 除此之外，还有一个操作叫做 tensor.unsqueeze_，tensor.unsqueeze_ 是 in_place 操作，会对自己进行改变，而 tensor.unsqueeze（或 torch.unsqueeze）不会对使用 unsqueeze 的 tensor 进行改变，想要获取 unsqueeze 后的值必须赋予个新值 tensor.expand 扩大张量 返回张量的一个新视图，可以将张量的单个维度扩大为更大的尺寸，新增加的维度将附在前面，扩大张量不需要分配新内存，仅仅是新建一个张量的视图 torch.expand()，只能把维度为 1 的拓展成指定维度。如果哪个维度为-1，就是该维度不变 >>> x = torch.tensor([[1], [2], [3]]) # torch.Size([3, 1]) >>> x = x.expand(3, 4) tensor([[1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.]]) 与此类似的还有 tensor.expand_as >>> a = torch.tensor([[2],[3],[4]]) # torch.Size([3, 1]) >>> b = torch.tensor([[2,2],[3,3],[5,5]]) # torch.Size([3, 2]) >>> a.expand_as(b) tensor([[2, 2], [3, 3], [4, 4]]) torch.flatten 压平张量 tensor.repeat 沿着指定的维度重复张量。不同于 expand() 方法，repeat() 复制的是张量中的数据 tensor.view 返回一个有相同数据但是不同形状的新的向量 交换维度 transpose 和 permute 都可以用于交换张量的维度，但是 transpose 交换多维张量的两个维度，permute 置换张量维度 >>> x = torch.zeros((4, 128, 512)) >>> x.permute(1, 0, 2).shape torch.Size([128, 4, 512]) >>> x.transpose(0, 1).shape torch.Size([128, 4, 512]) >>> x.permute(2, 0, 1).shape torch.Size([512, 4, 128]) 在 pytorch 中 view 函数的作用为重构张量的维度，相当于 numpy 中 resize 的功能，但是用法可能不太一样。view 把原先 tensor 中的数据按照行优先的顺序排成一个一维的数据，然后按照 view 的输入参数组合成其他维度的tensor 比如说是不管你原先的数据是 $[[[1,2,3],[4,5,6]]]$ 还是 $[1,2,3,4,5,6]$ ，因为它们排成一维向量都是 6 个元素，所以只要 view 后面的参数一致，得到的结果都是一样的 >>> a = torch.Tensor([[[1,2,3],[4,5,6]]]) >>> b = torch.Tensor([1,2,3,4,5,6]) >>>a.view(3, 2) tensor([[1., 2.], [3., 4.], [5., 6.]]) >>> b.view(3, 2) tensor([[1., 2.], [3., 4.], [5., 6.]]) tensor.sum tensor.sum() 可以按照指定的维度进行求和，但是有些时候对于 $N\\times C\\times H\\times W$ 这种类型的的输入，我们有奇葩要求通过 sum 得到 $N\\times C\\times 1\\times 1$ 的话，同样是可以通过 tensor.sum() 得到的，如下所示 a = torch.ones(size=(1, 3, 4, 4)) # 全 1 矩阵 b = a.sum(axis=[2, 3], keepdim=False) print(b.shape) # (1, 3) print(b) # [[16, 16, 16]] F.softmax 和 F.log_softmax log_softmax 就是在 softmax 的结果上多做一次 log 运算，如下公式所示 \r \\begin{align}\r L_{i}&=log\\frac{e^{z_i}}{\\sum_{j}^{K}{e^{z^j}}}\\\\\r &=log\\frac{e^{z_i}/e^{M}}{\\sum_{j}^{K}{e^{z^j/e^{M}}}}=log\\frac{e^{(z_i-M)}}{\\sum_{j}^{K}{e^{(z^j-M)}}}=(z_i-M)-log(\\sum_{j}^{K}{e^{(z^j-M)}})\r \\end{align}\r 这么做的目的是能够解决 softmax 函数 overflow 和 underflow，加快运算速度，提高数据稳定性 softmax 会进行指数操作，当输入较大的时候就会产生上溢出的情况，同理当输入为负数且绝对值很大的时候，就使得分子、分母变得极小，有可能四舍五入为零，导致下溢出 由此在一些教程中会出现将 log_softmax 的结果拿来 torch.exp()，目的即使如此为了防止溢出 CrossEntropyLoss 先说下交叉熵损失函数的计算公式 \r loss(x,cls)=-log\\left(\\frac{\\exp(x_{cls})}{\\sum_j\\exp(x_j)}\\right)=-x_{cls}+\\log\\left(\\sum_j\\exp(x_j)\\right)\r 我们下面来手动模拟一下如何计算交叉熵损失 output = torch.randn(1, 5, requires_grad=True) # 5 classes label = torch.empty(1, dtype=torch.long).random_(5) # gt label # Manual calculation first_item = 0 for i in range(1): first_item = -output[i][label[i]] second_item = 0 for i in range(1): for j in range(5): second_item += math.exp(output[i][j]) CrossEn_loss = 0 CrossEn_loss = (first_item + math.log(second_item)) # tensor(0.5891, grad_fn=) 下面用 nn.CrossEntropyLoss() 验算一遍，没有任何问题 # PyTorch function criterion = nn.CrossEntropyLoss() loss = criterion(output, label) # tensor(0.5891, grad_fn=) 我们给出 nn.CrossEntropyLoss() 的官方说明和样例 # Example of target with class indices loss = nn.CrossEntropyLoss() input = torch.randn(3, 5, requires_grad=True) target = torch.empty(3, dtype=torch.long).random_(5) output = loss(input, target) output.backward() # Example of target with class probabilities input = torch.randn(3, 5, requires_grad=True) target = torch.randn(3, 5).softmax(dim=1) output = loss(input, target) output.backward() 还有一种实现交叉熵损失的方法，该方法在 CLIP4Clip 工作中使用，用于计算 $N\\times N$ sim matrix 的交叉熵损失 class CrossEn(nn.Module): def __init__(self,): super(CrossEn, self).__init__() def forward(self, sim_matrix): logpt = F.log_softmax(sim_matrix, dim=-1) logpt = torch.diag(logpt) nce_loss = -logpt sim_loss = nce_loss.mean() return sim_loss 生成随机数 tensor 均匀分布 torch.rand(*sizes, out=None) 返回一个张量，包含了从区间 [0, 1) 的均匀分布中抽取的一组随机数。张量的形状由参数 sizes 定义 如，torch.rand(2, 3) 标准正态分布 torch.randn(*sizes, out=None) 返回一个张量，包含了从标准正态分布（均值为 0，方差为 1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数 sizes 定义 离散正态分布 torch.normal(means, std, out=None) 返回一个张量，包含了从指定均值 means 和标准差 std 的离散正态分布中抽取的一组随机数。标准差 std 是一个张量，包含每个输出元素相关的正态分布标准差。 如，torch.normal(mean=0.5, std=torch.arange(1, 6)) 线性间距向量 torch.linspace(start, end, steps=100, out=None) 返回一个 1 维张量，包含在区间 start 和 end 上均匀间隔的 step 个点。输出张量的长度由 steps 决定 如，torch.linspace(3, 10, steps=5) torch.optim 为了使用 torch.optim，我们需要构造一个 optimizer 对象，这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新 构建 optimizer 为了构建一个 optimizer ，需要给它一个包含了优化参数的 iterable。然后，可以设置 optimizer 的参数选项，比如学习率，权重衰减，等等 optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr = 0.0001) 为每个参数单独设置选项 optimizer 可以为每个车参数单独设置选项，这时传入 dict 的 iterable，每一个dict 都分别定义了一组参数，并且包含一个 param 键，这个键对应参数的列表 如， optim.SGD([{'params': model.base.parameters()}, {'params': model.classifier.parameters(), 'lr': 1e-3}], lr=1e-2, momentum=0.9) 这意味着 model.base的参数将会使用 1e-2 的学习率，model.classifier 的参数将会使用 1e-3 的学习率，并且 0.9 的 momentum 将会被用于所有的参数 关于为每个参数单独设置参数的问题，CLIP4Clip 工作给出了一个示例操作 param_optimizer = list(model.named_parameters()) no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] # Whole model decay params decay_param_tp = [(n, p) for n, p in param_optimizer if not any(nd in n for nd in no_decay)] # Whole model no decay params no_decay_param_tp = [(n, p) for n, p in param_optimizer if any(nd in n for nd in no_decay)] # CLIP model decay params decay_clip_param_tp = [(n, p) for n, p in decay_param_tp if \"clip.\" in n] # CLIP model no decay params no_decay_clip_param_tp = [(n, p) for n, p in no_decay_param_tp if \"clip.\" in n] # The rest part decay params decay_noclip_param_tp = [(n, p) for n, p in param_optimizer if \"clip.\" not in n] optimizer_grouped_parameters = [ {'params': [p for n, p in decay_clip_param_tp], 'weight_decay': weight_decay, 'lr': args.lr * coef_lr}, {'params': [p for n, p in no_decay_clip_param_tp], 'weight_decay': 0.0, 'lr': args.lr * coef_lr}, {'params': [p for n, p in decay_noclip_param_tp], 'weight_decay': weight_decay} ] scheduler = None optimizer = BertAdam(optimizer_grouped_parameters, lr=args.lr, warmup=args.warmup_proportion, schedule='warmup_cosine', b1=0.9, b2=0.98, e=1e-6, t_total=num_train_optimization_steps, weight_decay=weight_decay, max_grad_norm=1.0) 单步优化 所有的 optimizer 都实现了 step() 方法，这个方法会更新所有的参数。它能按两种方式来使用： optimizer.step()，这是大多数 optimizer 所支持的简化版本。一旦梯度被如 backward() 之类的函数计算好后，我们就可以调用这个函数 optimizer.step(closure)，一些优化算法例如 Conjugate Gradient 和 LBFGS 需要重复多次计算函数，因此需要传入一个闭包去允许它们重新计算模型。这个闭包应当清空梯度，计算损失，然后返回 如， for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure) named_parameters 和 parameters net.named_parameters() 返回的是一个生成器器，我们可以通过如下的代码查看网络的参数以及参数的值 for name, parms in net.named_parameters(): print('-->name:', name, '-->grad_requirs:', parms.requires_grad, '-->parms_value_mean:', parms.mean()) net.parameters() 和 net.named_parameters() 相似，区别在于：其输出只包含参数的值，不包含参数的所属信息。同时，如果我们想要知道网络有多少参数，而不关心具体的参数，我们可以将生成器用 list 转换一下，然后获取长度 len(list(net.parameters())) Pytorch 的分布式 初衷 我们在训练一个模型的时候总会考虑这样一些问题：如果训练数据量太大我们拆分成多份如何训练？如果模型网络太大我们拆分成多个片段如何训练？如果网络数据拆分在同一机子不同设备，或不同机子上不同设备上我们又如何训练？ 这就是为什么我们会需要 pytorch 的分布式，其实在 pytorch 训练过程中我们已经遇到一些关于分布式的包，如 THD C10D torch.multiprocessing torch.distributed DataParallel（DP） DistributedDataParallel（DDP） torch.distributed.rpc 下面我们就这些包展开详细的说明 torch.distributed PyTorch 的分布式依赖于 torch.distributed 模块，但是这个模块并非天然就包含在 PyTorch 库中。要启用 PyTorch distributed， 需要在源码编译的时候设置 USE_DISTRIBUTED = 1。目前在 Linux 系统上编译的时候，默认就是 USE_DISTRIBUTED = 1，因此默认就会编译 distributed 模块 torch.multiprocessing 为了并行的执行计算任务，一个直截了当的思想就是启动多个进程，然后使用 IPC 的方式进行信息交换——最方便的就是共享内存。PyTorch 的 multiprocessing 模块就来源于此，它封装了 python 原生的 multiprocessing 模块，并在 API 上做到了百分之百的兼容。在此基础上，它注册了定制的 reducers, 可以充分使用共享内存的 IPC 机制来让不同的进程对同一份数据进行读写： import torch.multiprocessing as mp from model import MyModel def train(model): for data, labels in data_loader: optimizer.zero_grad() loss_fn(model(data), labels).backward() optimizer.step() #会更新共享内存中的权重 if __name__ == '__main__': num_processes = 4 model = MyModel() #在下面fork新进程之前必须做share_memory的调用 model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(model,)) p.start() processes.append(p) for p in processes: p.join() 但是这种多进程的工作方式在遇到 CUDA 时有很多局限性，这导致了很多比较突兀的使用限制和代码编写方式：它规定了发送 tensor 的进程必须怎么怎么样、规定了接收 tensor 的进程必须怎么怎么样、规定了生产 tensor 的进程的生命周期必须怎样怎样、限制不能转发收到的 tensor 等等，以至于这些条件只要有一个没有遵守，在 CUDA 上的 multiprocessing 就会出现预期之外的行为。为了突破这些限制和掣肘， DataParallel 到来了 DataParallel 仅仅从模块的名字来看，DataParallel 也是为了解决 data 的并行问题的。DataParallel 是为了解决这样的问题：当输入的 batch 很大的时候，DataParallel 会将模型/网络复制运行到多个 CUDA 设备上，然后在输入的 batch 维度上进行切分，这里的切分就是 torch tensor 的 split() API 在 DataParallel 出生的年代，PyTorch 官方就开始推荐使用 nn.DataParallel 来代替 multiprocessing class Model(nn.Module): def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\"tIn Model: input size\", input.size(),\"output size\", output.size()) return output #数据集的长度为100，batch size为32，fc层的输入是5，输出是2 input_size = 5 output_size = 2 batch_size = 32 data_size = 100 model = Model(input_size, output_size) if torch.cuda.device_count() > 1: print(\"Gemfield have \", torch.cuda.device_count(), \"GPUs!\") model = nn.DataParallel(model) device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model.to(device) rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),batch_size=batch_size, shuffle=True) for data in rand_loader: input = data.to(device) output = model(input) print(\"Outside: input size\", input.size(),\"output_size\", output.size()) 本来 batch_size 是 32，但是由于使用了 DataParallel，而 Gemfield 有 2 个 GPU，因此一个 batch 被划分成了 2 份，也就是 tensor.split(16)，分别送往两个 GPU 上。值得注意的是：在第一次调用 model.to(device) 的时候，模型被加载到了第一个GPU设备上，而在第一次调用 output = model(input) 时候（也就是在进行 forward 的时候），模型被复制到了其余的 GPU 上，这里是第 2 个 GPU。程序输出如下（可见大小为 32 的batch被拆分成了大小为 16 的batch）： You have 2 GPUs! In Model: input size torch.Size([16, 5]) output size torch.Size([16, 2]) In Model: input size torch.Size([16, 5]) output size torch.Size([16, 2]) input size torch.Size([32, 5]) output_size torch.Size([32, 2]) In Model: input size torch.Size([16, 5]) output size torch.Size([16, 2]) In Model: input size torch.Size([16, 5]) output size torch.Size([16, 2]) input size torch.Size([32, 5]) output_size torch.Size([32, 2]) In Model: input size torch.Size([16, 5]) output size torch.Size([16, 2]) In Model: input size torch.Size([16, 5]) output size torch.Size([16, 2]) input size torch.Size([32, 5]) output_size torch.Size([32, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) input size torch.Size([4, 5]) output_size torch.Size([4, 2]) 总结 DataParallel 一次迭代的过程: DataLoader 把数据通过多个 worker 读到主进程的内存中； 通过 tensor 的 split 语义，将一个 batch 的数据切分成多个更小的 batch，然后分别送往不同的 CUDA 设备； 在不同的 CUDA 设备上完成前向计算，网络的输出被 gather 到主 CUDA 设备上（初始化时使用的设备），loss 而后在这里被计算出来； loss 然后被 scatter 到每个 CUDA 设备上，每个 CUDA 设备通过 BP 计算得到梯度； 然后每个 CUDA 设备上的梯度被 reduce 到主 CUDA 设备上，然后模型权重在主 CUDA 设备上获得更新； 在下一次迭代之前，主 CUDA 设备将模型参数 broadcast 到其它 CUDA 设备上，完成权重参数值的同步 DataParallel 通过复制一个网络到多个 CUDA 设备，然后再 split 一个 batch 的 data 到多个 cuda 设备，通过这种并行计算的方式解决了 batch 很大的问题，但也有自身的不足： 它无法跨越机器，DataParallel 是单进程多线程的，无法在多个机器上工作； 它基于多线程的方式，确实方便了信息的交换，但受困于 GIL； 数据集先拷贝到主进程，然后再 split 到每个 CUDA 设备上； 权重参数只在主 CUDA 上更新，需要每次迭代前向所有的 CUDA 设备做一次同步； 每次迭代的网络输出需要 gather 到主 CUDA 设备上； 如果模型太大需要使用 model parallel 的时候，DataParallel 目前还不支持； 这个时候，DistributedDataParallel 来了，并且自此之后，不管是单机还是多机，我们都推荐使用 DDP 来代替 DP（DataParallel） DistributedDataParallel(DDP) DDP基于 torch.distributed 模块，要使用 DDP，需要先熟悉两个概念：Backend 和initialization methods initialization methods 因为 DDP 是真正的分布式，可以使用多台机器来组成一次并行运算的任务，所以就需要一种方法来给大家传递一个信息——如何联系到其它机器上的进程？目前 DDP 模块支持 3 种initialization methods：TCP initialization；Shared file-system initialization；Environment variable initialization Backend 既然能够在不同的进程间进行通信，那必然是依赖于一些 IPC 的通信机制，这些通信机制一般是由 PyTorch 之外的三方实现的。在 distributed 模块中，一共有 4 种不同的 IPC 通信backend： TCP：TCP backend 已经被废弃了； MPI：如果是MPI backend，则不需要指定world_size和rank，因为这两个值由MPI runtime来分配和维护； Gloo：如果是 CPU 的训练，则使用 MPI 或 Gloo，推荐使用 Gloo； NCCL：DDP 只支持Gloo 和 NCCL，如果是GPU训练的话，使用NCCL，同时，只有 NCCL 支持 InfiniBand 和 GPUDirect； 下面我们以一个例子来看下 DDP 的使用 Gemfield 的 MLab2.0 环境上使用的是 CUDA 设备，因此使用 NCCL backend；又因为没有 IB 连接，因此需要设置如下环境变量来禁用 IB 转而使用以太网（也可以不用设置，NCCL backend 会自动找寻，当然，如果自动找寻失败了，最好就手工来设置）： 1）设置环境变量 export NCCL_SOCKET_IFNAME=eth0 export NCCL_IB_DISABLE=1 将 NCCL_IB_DISABLE 设置为 1 来禁止使用 InfiniBand，转而使用 IP；如果网络接口不能被自动发现，则手工设置 NCCL_SOCKET_IFNAME 2）启动 master 进程 启动 rank 为 0 的进程（这里我们使用了 tcp 的初始化方法），我们在 node 0 上启动 python gemfield_ddp.py -a resnet50 --dist-url 'tcp://172.16.90.44:27030' --gpu 0 --world-size 2 --rank 0 [your_dataset_folder] DDP 的 MPI 风格的四元组来了：初始化方法、backend、world size、rank 这里使用了 tcp 的初始化方法，使用了 node 0 上的第一块 GPU，world-size 为 2 说明我们 group 中一共要启动 2 个进程，rank 为 0 说明这是第 1 个进程（也就是 master 进程）。只要 group 中的进程数还不够，进程就会阻塞在 init_process_group 调用上。因此，这条命令执行后，master 进程就处于等待状态 3）启动其它非 master 进程 然后在 node 1 上启动 rank 为 1 的进程 python gemfield_ddp.py -a resnet50 --dist-url 'tcp://172.16.90.44:27030' --gpu 0 --world-size 2 --rank 1 [your_dataset_folder] 在 gemfield_ddp.py 的脚本源码中，可以看到里面默认使用了 NCCL 的 backend，使用了 32 的 batch-size（那么在这种情况下，整体就相当于 batchsize = 32 * 2 = 64）。因为此时 process group 中的进程数量达到了 world size，因此训练就开始往前迭代了 gemfield_ddp.py 一共在 4 处地方引用到了 DDP 的 API： torch.distributed.init_process_group(backend='nccl', init_method=args.dist_url, world_size=args.world_size, rank=args.rank) 这是初始化进程组，参数正好是上面提到的 MPI 风格的四元组； model=torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu]) 也就是我们的网络需要被 DistributedDataParallel wrap 起来，DDP 封装了分布式计算通信原语，这样 DDP 后的 model 看起来如同之前的 model 一样整洁；被 DDP 封装的 model 的参数梯度才会进行 all reduce； train_sampler=torch.utils.data.distributed.DistributedSampler(train_dataset) 我们需要 DistributedSampler 作为实例传递给 DataLoader 来配合 DDP 使用，这样数据集的样本会为每个进程划分，每个进程读取各自的样本 train_sampler.set_epoch(epoch) set_epoch 是在 DDP 模式下 shuffle 数据集的方式 当上面横跨 2 个 node 上的训练进程开始工作起来后，一次 DistributedDataParallel 迭代中的步骤如下所示： process group 中的训练进程都起来后，rank 为 0 的进程会将网络初始化参数 broadcast 到其它每个进程中，确保每个进程中的网络都是一样的初始化的值（默认行为，你也可以通过参数禁止）； 每个进程各自读取各自的训练数据，DistributedSampler 确保了进程两两之间读到的是不一样的数据； 前向和 loss 的计算如今都是在每个进程上（也就是每个 CUDA 设备上）独立计算完成的；网络的输出不再需要 gather 到 master 进程上了，这和 DP 显著不一样； 反向阶段，梯度信息通过 allReduce 的 MPI 原语，将每个进程中计算到的梯度 reduce 到每个进程；也就是 backward 调用结束后，每个进程中的 param.grad 都是一样的值；注意，为了提高 allReduce 的效率，梯度信息被划分成了多个 buckets； 更新模型参数阶段，因为刚开始模型的参数是一样的，而梯度又是 all reduced 的，这样更新完模型参数后，每个进程/设备上的权重参数也是一样的。因此，就无需 DP 那样每次迭代后需要同步一次网络参数，这个阶段的 broadcast 操作就不存在了！注意，Network 中的 Buffers (比如 BatchNorm 数据) 需要在每次迭代中从 rank 为 0 的进程 broadcast 到进程组的其它进程上 ModelParallel model parallel tutorial 本文的前述部分主要说的是如何更加并发的处理更多的输入数据，但是如果一个网络本身太大，以至于一个cuda设备都装不下怎么办？那就是 model parallel 了 ModelParallel 的核心思想就是把一个网络拆分成不同的部分，然后分别运行在不同的 CUDA 设备上。要达到这一点，就需要改造网络的构造和 forward 部分。引用官方文档里的一个例子，就是： import torch import torch.nn as nn import torch.optim as optim class ToyModel(nn.Module): def __init__(self): super(ToyModel, self).__init__() self.net1 = torch.nn.Linear(10, 10).to('cuda:0') self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10, 5).to('cuda:1') def forward(self, x): x = self.relu(self.net1(x.to('cuda:0'))) return self.net2(x.to('cuda:1')) 可以看到 ModelParallel 的关键之处有2点： ToyModel 内部的不同 layer 分别放在了不同的 CUDA 设备上； forward 中一个 layer 的输出结果需要通过 tensor.to 的语义 copy 到另一个 layer 所在的 CUDA 设备上 我们并没有提及反向和优化器，就是因为 PyTorch 的 backward 和 optim 优化器模块可以应付这种情况。唯一需要注意的就是，在调用 loss 函数的时候，网络的输出和 label 需要放在一个CUDA设备上，如下所示 model = ToyModel() loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.001) optimizer.zero_grad() outputs = model(torch.randn(20, 10)) labels = torch.randn(20, 5).to('cuda:1') loss_fn(outputs, labels).backward() optimizer.step() torch.distributed 我们一般通过如下的命令形式进行分布式训练 python -m torch.distributed.launch --nproc_per_node=4 tools/train.py --cfg xxx.yaml python -m torch.distributed.launch --nproc_per_node=4 表示调用 torch.distributed.launch 这个.py 文件进行分布式训练 --nproc_per_node=4 说明创建节点数为 4，这个值通常与训练使用的 GPU 数量一致；严格来说，nproc_per_node 表示每个节点上有多少个进程，一般每个进程独占一块 GPU，但这不是绝对的，看 tools/train.py --cfg xxx.yaml 是真正的训练文件，后面的 --cfg xxx.yaml 是 train.py 使用时需要给出的执行参数名称和值 Pytorch 半精度浮点数 如果网络要在 GPU上跑，模型和输入样本数据都要cuda().half()；模型参数转换为 HALF 型，不必索引到每层，直接 model.cuda().half() 即可 另外关于半精度和单精度浮点型对训练速度来说是存在差异的，随着网络参数和 epoch 的增加，半精度和全精度在训练上的时间差就表现出来了 torchvision.transforms torchvision 是 pytorch 的一个图形库，它服务于 PyTorch 深度学习框架的，主要用来构建计算机视觉模型。以下是torchvision的构成： torchvision.datasets：一些加载数据的函数及常用的数据集接口； torchvision.models：包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等； torchvision.transforms：常用的图片变换，例如裁剪、旋转等； torchvision.utils：其他的一些有用的方法。 Compose() 类的主要作用是串联多个图片变换的操作，使用如下 transforms.Compose([ transforms.CenterCrop(10), transforms.ToTensor(), ]) 事实上，Compose() 类会将 transforms 列表里面的 transform 操作进行遍历。实现的代码很简单： ## 这里对源码进行了部分截取。 def __call__(self, img): for t in self.transforms: img = t(img) return img torch.nn 和 nn.functional 的区别 两者的相同之处： nn. 和 nn.functional. 的实际功能是相同的，即 nn.Conv2d 和 nn.functional.conv2d 都是进行卷积，nn.Dropout 和 nn.functional.dropout 都是进行dropout 运行效率也是近乎相同 nn.functional. 是函数接口，而 nn. 是 nn.functional. 的类封装，并且 nn. 都继承于一个共同祖先nn.Module，这一点导致 nn. 除了具有 nn.functional. 功能之外，内部附带了 nn.Module 相关的属性和方法，例如 train(), eval(),load_state_dict, state_dict 等 两者的差别之处： 1）两者的调用方式不同 nn. 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据 inputs = torch.rand(64, 3, 244, 244) conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1) out = conv(inputs) nn.functional. 同时传入输入数据和 weight，bias 等其他参数 weight = torch.rand(64,3,3,3) bias = torch.rand(64) out = nn.functional.conv2d(inputs, weight, bias, padding=1) 2）nn. 继承于 nn.Module， 能够很好的与 nn.Sequential 结合使用， 而 nn.functional. 无法与 nn.Sequential 结合使用 fm_layer = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.BatchNorm2d(num_features=64), nn.ReLU(), nn.MaxPool2d(kernel_size=2), nn.Dropout(0.2) ) 3）nn. 不需要你自己定义和管理 weight；而 nn.functional. 需要你自己定义 weight，每次调用的时候都需要手动传入 weight，不利于代码复用 使用 nn. 定义一个CNN class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,padding=0) self.relu1 = nn.ReLU() self.maxpool1 = nn.MaxPool2d(kernel_size=2) self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=0) self.relu2 = nn.ReLU() self.maxpool2 = nn.MaxPool2d(kernel_size=2) self.linear1 = nn.Linear(4 * 4 * 32, 10) def forward(self, x): x = x.view(x.size(0), -1) out = self.maxpool1(self.relu1(self.cnn1(x))) out = self.maxpool2(self.relu2(self.cnn2(out))) out = self.linear1(out.view(x.size(0), -1)) return out 使用 nn.functional. 定义一个同样的 CNN class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.cnn1_weight = nn.Parameter(torch.rand(16, 1, 5, 5)) self.bias1_weight = nn.Parameter(torch.rand(16)) self.cnn2_weight = nn.Parameter(torch.rand(32, 16, 5, 5)) self.bias2_weight = nn.Parameter(torch.rand(32)) self.linear1_weight = nn.Parameter(torch.rand(4 * 4 * 32, 10)) self.bias3_weight = nn.Parameter(torch.rand(10)) def forward(self, x): x = x.view(x.size(0), -1) out = F.conv2d(x, self.cnn1_weight, self.bias1_weight) out = F.relu(out) out = F.max_pool2d(out) out = F.conv2d(x, self.cnn2_weight, self.bias2_weight) out = F.relu(out) out = F.max_pool2d(out) out = F.linear(x, self.linear1_weight, self.bias3_weight) return out 上面两种定义方式得到 CNN 功能都是相同的，至于喜欢哪一种方式，是个人口味问题，但 PyTorch 官方推荐： 具有学习参数的（例如，conv2d, linear, batch_norm)采用 nn. 方式 没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用 nn.functional. 或者 nn. 方式 但关于dropout，个人强烈推荐使用 nn. 方式，因为一般情况下只有训练阶段才进行 dropout，在 eval 阶段都不会进行 dropout。使用 nn. 方式定义 dropout，在调用 model.eval() 之后，model 中所有的 dropout layer 都关闭，但以 nn.function.dropout 方式定义 dropout，在调用 model.eval() 之后并不能关闭dropout class Model1(nn.Module): def __init__(self): super(Model1, self).__init__() self.dropout = nn.Dropout(0.5) def forward(self, x): return self.dropout(x) class Model2(nn.Module): def __init__(self): super(Model2, self).__init__() def forward(self, x): return F.dropout(x) m1 = Model1() m2 = Model2() inputs = torch.rand(10) print(m1(inputs)) print(m2(inputs)) print(20 * '-' + \"eval model:\" + 20 * '-' + '\\r\\n') m1.eval() m2.eval() print(m1(inputs)) print(m2(inputs)) 从上面输出可以看出 m2 调用了 eval 之后，dropout 照样还在正常工作 当然如果你有强烈愿望坚持使用 nn.functional.dropout，也可以采用下面方式来补救 class Model3(nn.Module): def __init__(self): super(Model3, self).__init__() def forward(self, x): return F.dropout(x, training=self.training) 关于 tricks 实现 读取视频长度 正常大小视频 import cv2 def get_video_duration(filename): \"\"\" python获取视频文件时长 \"\"\" cap = cv2.VideoCapture(filename) if cap.isOpened(): rate = cap.get(5) frame_num =cap.get(7) duration = frame_num/rate return duration return -1 超长视频 有些时候会出现读取的视频时长在一两个小时左右，这种视频的读取就不能采用上述方法，因为 frame_num =cap.get(7) 这一步会因为读取的帧过多而引发如下的错误 [mov,mp4,m4a,3gp,3g2,mj2 @ 0x558dd14af680] Referenced QT chapter track not f 我们提供第二种方法 from moviepy.editor import VideoFileClip def get_long_video_duration(path): clip = VideoFileClip(path) return clip.duration 初始化模型参数 方法一 先定义初始化模型方法，运用 apply() class Net(nn.Module): def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim): super().__init__() self.layer = nn.Sequential( nn.Linear(in_dim, n_hidden_1), nn.ReLU(True), nn.Linear(n_hidden_1, n_hidden_2), nn.ReLU(True), nn.Linear(n_hidden_2, out_dim) # 将weight_init应用在子模块上 self.apply(self.weight_init) #torch中的apply函数通过可以不断遍历model的各个模块。实际上其使用的是深度优先算法 def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x # 根据网络层的不同定义不同的初始化方式 def weight_init(self, m): if isinstance(m, nn.Linear): nn.init.xavier_normal_(m.weight) nn.init.constant_(m.bias, 0) # 也可以判断是否为conv2d，使用相应的初始化方式 elif isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') # 是否为批归一化层 elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) 方法二 定义在模型中，利用 self.modules() 来进行循环 class Net(nn.Module): def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim): super().__init__() self.layer = nn.Sequential( nn.Linear(in_dim, n_hidden_1), nn.ReLU(True), nn.Linear(n_hidden_1, n_hidden_2), nn.ReLU(True), nn.Linear(n_hidden_2, out_dim) ) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x 保存/加载模型 在 Pytorch 中一种模型保存和加载的方式如下 # save torch.save(model.state_dict(), PATH) # load model = MyModel(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.eval() model.state_dict() 返回的是一个 OrderDict，存储了网络结构的名字和对应的参数，下面看看源代码如何实现的 # torch.nn.modules.module.py class Module(object): def state_dict(self, destination=None, prefix='', keep_vars=False): if destination is None: destination = OrderedDict() destination._metadata = OrderedDict() destination._metadata[prefix[:-1]] = local_metadata = dict(version=self._version) for name, param in self._parameters.items(): if param is not None: destination[prefix + name] = param if keep_vars else param.data for name, buf in self._buffers.items(): if buf is not None: destination[prefix + name] = buf if keep_vars else buf.data for name, module in self._modules.items(): if module is not None: module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars) for hook in self._state_dict_hooks.values(): hook_result = hook(self, destination, prefix, local_metadata) if hook_result is not None: destination = hook_result return destination 可以看到 state_dict 函数中遍历了 4 种元素，分别是 _paramters，_buffers，_modules 和 _state_dict_hooks， _parameters 是 nn.Module 在 __init__() 函数中就定义了的一个OrderDict 类，每当我们给一个成员变量定义一个 nn.parameter.Paramter 的时候，都会自动注册到 _parameters _modules 包含了一个类所有的派生于 Module 的成员，但是如果成员被封装到列表中，并不会被添加到 _modules 中。如有必要，可以使用 ModuleList 替代列表来使用，ModuleList 继承了类，实现了 list 的功能 _buffers 的填充是通过 register_buffer API 来完成的，通常用来将一些需要持久化的状态（但又不是网络的参数）放到 _buffer 里；一些极其个别的操作，比如 BN，会将 running_mean 的值放入进来 最后一种就是在读取 state_dict 时希望执行的操作，一般为空，所以不做考 需要注意的是，在读取 Module 时采用的递归的读取方式，并且名字间使用 . 做分割，以方便后面 load_state_dict 读取参数 load_state_dict 可以分两部分来看： load 函数会递归地对模型进行参数恢复，首先我们需要明确 state_dict 这个变量表示你之前保存的模型参数序列，而 _load_from_state_dict 函数中的 local_state 表示你的代码中定义的模型的结构。举个例子， _load_from_state_dict 的作用就可以简单理解对一个名为 conv.weight 的子模块做参数恢复，那么就以递归的方式先判断 conv 是否在 state__dict 和 local_state 中，如果不在就把 conv 添加到 unexpected_keys 中去，否则递归的判断 conv.weight 是否存在，如果都存在就执行 param.copy_(input_param) ，这样就完成了 conv.weight 的参数拷贝 if strict 这个部分的作用是判断上面参数拷贝过程中是否有 unexpected_keys 或者 missing_keys ，如果有就报错，代码不能继续执行。当然，如果 strict=False，则会忽略这些细节。 def load_state_dict(self, state_dict, strict=True): missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _load_from_state_dict can modify it metadata = getattr(state_dict, '_metadata', None) state_dict = state_dict.copy() if metadata is not None: state_dict._metadata = metadata def load(module, prefix=''): local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {}) module._load_from_state_dict( state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) for name, child in module._modules.items(): if child is not None: load(child, prefix + name + '.') load(self) if strict: error_msg = '' if len(unexpected_keys) > 0: error_msgs.insert( 0, 'Unexpected key(s) in state_dict: {}. '.format( ', '.join('\"{}\"'.format(k) for k in unexpected_keys))) if len(missing_keys) > 0: error_msgs.insert( 0, 'Missing key(s) in state_dict: {}. '.format( ', '.join('\"{}\"'.format(k) for k in missing_keys))) if len(error_msgs) > 0: raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format( self.__class__.__name__, \"\\n\\t\".join(error_msgs))) def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs): for hook in self._load_state_dict_pre_hooks.values(): hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) local_name_params = itertools.chain(self._parameters.items(), self._buffers.items()) local_state = {k: v.data for k, v in local_name_params if v is not None} for name, param in local_state.items(): key = prefix + name if key in state_dict: input_param = state_dict[key] # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+ if len(param.shape) == 0 and len(input_param.shape) == 1: input_param = input_param[0] if input_param.shape != param.shape: # local shape should match the one in checkpoint error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, ' 'the shape in current model is {}.' .format(key, input_param.shape, param.shape)) continue if isinstance(input_param, Parameter): # backwards compatibility for serialized parameters input_param = input_param.data try: param.copy_(input_param) except Exception: error_msgs.append('While copying the parameter named \"{}\", ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {}.' .format(key, param.size(), input_param.size())) elif strict: missing_keys.append(key) if strict: for key, input_param in state_dict.items(): if key.startswith(prefix): input_name = key[len(prefix):] input_name = input_name.split('.', 1)[0] # get the name of param/buffer/child if input_name not in self._modules and input_name not in local_state: unexpected_keys.append(key) 梯度下降的框架 我们看一个 bp 回归的梯度下降代码，也是常用梯度下降的一个框架写法 for epoch in range(num_epoch): for i, data in enumerate(train_loader): # 用 optimizer 将 model 的梯度归零 optimizer.zero_grad() # 调用 model 的 forward 函数预测 train_pred = model(data[0].cuda()) # 计算 loss，注意，prediction 和 label 必须同时在 CPU 或 GPU 上 batch_loss = loss(train_pred, data[1].cuda()) # 利用反向传播计算参数的梯度 batch_loss.backward() # 通过 optimizer 根据梯度更新参数 optimizer.step() 避免梯度更新 torch.no_grad() with torch.no_grad(): # 具体操作 上面的具体操作将都不更新梯度，一般用于验证或测试阶段 param.requires_grad p.requires_grad=False 一般用于将某一层设置为自动更新梯度，以避免训练模型时对该层进行参数调整 model.eval() 模型支持 train 模式和 eval 模式，在使用模型之前调用 model.eval()，进入 eval 评估模型，它将改变 forward，如禁止 dropout，并用统计数据做 batch norm。因此，有时 train 模式和 eval 模式模型计算的结果不同 可视化网络 一种简单的方式是直接打印网络，如 print(model) AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ) 另外一种方式是使用 torchsummary，可以更直观，排版也更好看一些 如， import torch, torchvision from torchsummary import summary model = torchvision.models.vgg.vgg16() summary(model, (3, 224, 224)) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 55, 55] 23,296 ReLU-2 [-1, 64, 55, 55] 0 MaxPool2d-3 [-1, 64, 27, 27] 0 Conv2d-4 [-1, 192, 27, 27] 307,392 ReLU-5 [-1, 192, 27, 27] 0 MaxPool2d-6 [-1, 192, 13, 13] 0 Conv2d-7 [-1, 384, 13, 13] 663,936 ReLU-8 [-1, 384, 13, 13] 0 Conv2d-9 [-1, 256, 13, 13] 884,992 ReLU-10 [-1, 256, 13, 13] 0 Conv2d-11 [-1, 256, 13, 13] 590,080 ReLU-12 [-1, 256, 13, 13] 0 MaxPool2d-13 [-1, 256, 6, 6] 0 AdaptiveAvgPool2d-14 [-1, 256, 6, 6] 0 Dropout-15 [-1, 9216] 0 Linear-16 [-1, 4096] 37,752,832 ReLU-17 [-1, 4096] 0 Dropout-18 [-1, 4096] 0 Linear-19 [-1, 4096] 16,781,312 ReLU-20 [-1, 4096] 0 Linear-21 [-1, 1000] 4,097,000 ================================================================ Total params: 61,100,840 Trainable params: 61,100,840 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.57 Forward/backward pass size (MB): 8.38 Params size (MB): 233.08 Estimated Total Size (MB): 242.03 ---------------------------------------------------------------- Tensorboard PyTorch 比较高一点的版本都集成了 tensorboard，使用 from torch.utils.tensorboard import SummaryWriter 即可 TensorboardX 使得 TensorFlow 以外的其他神经网络框架（如我们这里使用的 PyTorch）也可以使用到 Tensorboard 的便捷功能 要使用 TensorboardX 首先就需要创建一个 SummaryWriter，下面展示了三种初始化 SummaryWriter 的方法 from tensorboardX import SummaryWriter # Creates writer1 object. # The log will be saved in 'runs/exp' writer1 = SummaryWriter('runs/exp') # Creates writer2 object with auto generated file name # The log directory will be something like 'runs/Aug20-17-20-33' writer2 = SummaryWriter() # Creates writer3 object with auto generated file name, the comment will be appended to the filename. # The log directory will be something like 'runs/Aug20-17-20-33-resnet' writer3 = SummaryWriter(comment='resnet') 接下来，我们就可以调用 SummaryWriter 实例的各种 add_something 方法向日志中写入不同类型的数据 想要在浏览器中查看可视化这些数据，只要在命令行中开启 tensorboard 即可：tensorboard --logdir= 其中的 既可以是单个 run 的路径，如上面 writer1 生成的 runs/exp；也可以是多个 run 的父目录，如 runs/ 下面可能会有很多的子文件夹，每个文件夹都代表了一次实验，我们令 --logdir=runs/ 就可以在 tensorboard 可视化界面中方便地横向比较 runs/ 下不同次实验所得数据的差异 下面我们了解一下各种数据记录方式： add_scalar：add_scalar(tag, scalar_value, global_step=None, walltime=None)，通常我们会用标量来记录 loss、参数梯度均值或参数均值。对于分 epoch 的情况，我们可以使用 epoch * all_steps_per_epoch + step 来设置 global_step，这样可以保证可视化的曲线是连续的 add_image：add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')，用于记录单个图像数据。注意，该方法需要 pillow 库的支持。img_tensor（torch.Tensor/numpy.array）存储图像数据，dataformats (string，optional) 控制图像数据的格式，默认为 CHW，即 Channel × Height × Width，当然也可以设定为 HWC、HW 等。我们一般会使用 add_image 来实时观察生成式模型的生成效果，或者可视化分割、目标检测的结果，帮助调试模型。add_image 的方法一次只能插入一张图片，如果要一次插入多张图片，有下面两种方法 使用 torchvision 中的 make_grid 方法将多张图片拼合成一张图片后，再调用 add_image 方法 使用 SummaryWriter 的 add_images 方法 ，参数和 add_image 类似 add_histogram：add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)，可以用于观察数据、训练参数、特征的直方图，了解到它们大致的分布情况，辅助神经网络的训练过程 add_graph：add_graph(model, input_to_model=None, verbose=False, **kwargs)，用于可视化神经网络模型 add_embedding：add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) 用于在二维或三维空间可视化 embedding 向量。add_embedding 是一个很实用的方法，不仅可以将高维特征使用 PCA、t-SNE 等方法降维至二维平面或三维空间显示，还可观察每一个数据点在降维前的特征空间的 K 近邻情况 下面是取自 TensorboardX 官方文档 的一个样例 # demo.py import torch import torchvision.utils as vutils import numpy as np import torchvision.models as models from torchvision import datasets from tensorboardX import SummaryWriter resnet18 = models.resnet18(False) writer = SummaryWriter() sample_rate = 44100 freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440] for n_iter in range(100): dummy_s1 = torch.rand(1) dummy_s2 = torch.rand(1) # data grouping by `slash` writer.add_scalar('data/scalar1', dummy_s1[0], n_iter) writer.add_scalar('data/scalar2', dummy_s2[0], n_iter) writer.add_scalars('data/scalar_group', {'xsinx': n_iter * np.sin(n_iter), 'xcosx': n_iter * np.cos(n_iter), 'arctanx': np.arctan(n_iter)}, n_iter) dummy_img = torch.rand(32, 3, 64, 64) # output from network if n_iter % 10 == 0: x = vutils.make_grid(dummy_img, normalize=True, scale_each=True) writer.add_image('Image', x, n_iter) dummy_audio = torch.zeros(sample_rate * 2) for i in range(x.size(0)): # amplitude of sound should in [-1, 1] dummy_audio[i] = np.cos(freqs[n_iter // 10] * np.pi * float(i) / float(sample_rate)) writer.add_audio('myAudio', dummy_audio, n_iter, sample_rate=sample_rate) writer.add_text('Text', 'text logged at step:' + str(n_iter), n_iter) for name, param in resnet18.named_parameters(): writer.add_histogram(name, param.clone().cpu().data.numpy(), n_iter) # needs tensorboard 0.4RC or later writer.add_pr_curve('xoxo', np.random.randint(2, size=100), np.random.rand(100), n_iter) dataset = datasets.MNIST('mnist', train=False, download=True) images = dataset.test_data[:100].float() label = dataset.test_labels[:100] features = images.view(100, 784) writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1)) # export scalar data to JSON for external processing writer.export_scalars_to_json(\"./all_scalars.json\") writer.close() 第三方工具 plotly 官网 Altair 官网 Streamlit 官网 "},"【开发手册】Linux.html":{"url":"【开发手册】Linux.html","title":"Part One. Linux 开发手册","keywords":"","body":"第零章 开发手册 df 命令 df 命令可以查看磁盘驱动当前的可用空间，下面可以看到 df 和 df -hl 之间的一些差异，相对来说用 df -hl 更方便我们阅读 命令 说明 df -h 查看每个根路径的分区大小 du -sh [目录名] 返回该目录的大小 du -sm [文件夹] 返回该文件夹总兆数 du -h [目录名] 查看指定文件夹下的所有文件大小（包含子文件夹） nohup 命令 nohup 英文全称 no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。nohup 命令，在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中 例如，在后台执行 root 目录下的 runoob.sh 脚本 nohup /root/runoob.sh & 在终端如果看到以下输出说明运行成功： appending output to nohup.out 这时我们打开 root 目录 可以看到生成了 nohup.out 文件 如果要停止运行，你需要使用以下命令查找到 nohup 运行脚本到 PID，然后使用 kill 命令来删除： ps -aux | grep \"runoob.sh\" 以下命令在后台执行 root 目录下的 runoob.sh 脚本，并重定向输入到 runoob.log 文件： nohup /root/runoob.sh > runoob.log 2>&1 & 2>&1 解释：将标准错误 2 重定向到标准输出 &1 ，标准输出 &1 再被重定向输入到 runoob.log 文件中 删除文件夹和文件 -r 就是向下递归，不管有多少级目录，一并删除 -f 就是直接强行删除，不作任何提示的意思 删除文件夹实例： rm -rf /var/log/httpd/access 将会删除/var/log/httpd/access目录以及其下所有文件、文件夹 删除文件使用实例： rm -f /var/log/httpd/access.log 将会强制删除/var/log/httpd/access.log这个文件 运行 .sh 文件 一种常见的方法就是 sh xxx.sh，这样子 xxx.sh 文件可以没有 x 权限 另外一种方法是 ./xxx.sh，这种方式要求 xxx.sh 必须有 x 权限，可以通过 chmod u+x xxx.sh 来给文件添加 x 权限 CUDA 相关 查看驱动版本、CUDA版本 nvidia-smi 中可以直接查看 或者通过 nvcc -V，这个指令要先安装好 nvidia-cuda-toolkit 再或者直接访问 CUDA 的版本文件 cat /usr/local/cuda/version.txt 查看使用显卡的进程 fuser -v /dev/nvidia* 或 lsof /dev/nvidia* screen 命令 Linux screen 命令用于多重视窗管理程序 创建 screen 终端 # screen //创建 screen 终端 创建 screen 终端 并执行任务 # screen vi ~/main.c //创建 screen 终端 ，并执行 vi命令 离开 screen 终端 # screen vi ~/main.c //创建 screen 终端 ，并执行 vi命令 #include main () { } \"~/mail.c\" 0,0-1 在 screen 终端下 按下 Ctrl+a d键 重新连接离开的 screen 终端 # screen -ls //显示已创建的screen终端 There are screens on: 2433.pts-3.linux (2013年10月20日 16时48分59秒) (Detached) 2428.pts-3.linux (2013年10月20日 16时48分05秒) (Detached) 2284.pts-3.linux (2013年10月20日 16时14分55秒) (Detached) 2276.pts-3.linux (2013年10月20日 16时13分18秒) (Detached) 4 Sockets in /var/run/screen/S-root. # screen -r 2276 //连接 screen_id 为 2276 的 screen终端 查看文件个数 查看当前目录下的文件个数 ls -l | grep \"^-\"| wc -l 查看指定目录下的文件个数 ls -l specified_dir | grep \"^-\"| wc -l 递归查询当前目录下的文件个数 所谓递归，即如果当前目录有文件夹，则会层层搜索所有文件夹中的所有文件 ls -lR | grep \"^-\"| wc -l 递归查询指定目录下的文件个数 ls -lR specified_dir | grep \"^-\"| wc -l 查看进程用户 在多用户下，如果希望查看对应 pid 的用户可以使用下面命令 ps aux | grep PID 同时，反过来，我们也可以通过 top 命令查看某用户的所有进程 top -u [username] 压缩 / 解压 tar.gz 压缩命令 tar -zcvf 压缩文件名.tar.gz 被压缩文件名 解压命令 tar -zxvf 压缩文件名.tar.gz 批量解压 用tar命令批量解压某个文件夹下所有的tar.gz文件 ls *.tar.gz | xargs -n1 tar xzvf 文件查找 在当前目录查找问价 find . -name \"rc.local\" 使用通配符 find . -name \"rc*\" 查看隐藏文件 ls -al 第一章 shell 是什么 shell 是一个接收由键盘输入的命令，并将其传递给操作系统执行的程序。通常，我们看到的终端都是终端仿真器（terminal emulator） 一般 shell 的提示符都会是的 $，如果是 # 则代表当前是超级用户 第二章 导航 在 Windows 系统中，每个存储设备都有一个独立的文件系统树。而在类 UNIX 系统中，如 Linux，无论多少驱动器或存储设备与计算机相连，通常只有一个文件系统树。根据系统管理员的设置，存储设备将会挂载到文件系统的不同位置 pwd 命令 使用 pwd 命令可以显示当前工作目录 ls 命令 使用 ls 命令可以列出当前工作目录的文件和目录 以 \".\" 字符开头的文件名是隐藏的，仅使用 ls 是不会列出这些文件，除非输入 ls -a，在创建用户账号时 ls 除了可以列出当前工作目录之外，还可以指定要显示的目录 甚至可以指定多个目录，下面的这例子就列出了用户主目录（由 \"~\" 表示）和 /usr 目录的内容 在命令中加上 -l 可以将输出以长格式显示，输出信息的详细解读可以参考 linux ls -l 详解 其中第一个字符表示文件类型，常见的文件类型在下面中给出 下面是长格式显示其他字段的说明 我们可能会看到带有如下条目的目录信息，该条目信息的第一个字母是 l，而且看起来像是有两个文件名，这种特殊的文件叫做符号链接（软链接） 同时还有一些 ls 命令常用的选项在下标列出 cd 命令 使用 cd 命令可以改变工目录，输入 cd，然后再输入目标工作目录的路径名即可。路径名可以分绝对路径和相对路径，在绝对路径中用前导斜杠表示根目录，/usr/bin 就表示根目录中有一个 usr 的目录，该目录还包含一个 bin 目录，通常大多数系统程序都安装到这个目录里 相对路径中会有两种特殊的符号点 \".\" 和点点 \"..\"，点代表工作目录，点点代表工作目录的父目录 cd 快捷方式 命令 说明 cd 将工作目录改变成主目录 (例如 /home/aries) cd - 将工作目录改变成向前的工作目录 du ~username 将工作目录改变为 username 的主目录 第三章 Linux 系统 我们还是接着讨论文件系统里面的文件，在 UNIX 系统中，有个普遍的观念：所有的东西都是一个文件，我们可以使用 file 命令来确定文件类型，例如下面 同时，在 Linux 系统中，大部分的系统设置文件都是文本文件，Linux 同时也提供了 less 命令来查看文本文件，例如，想要查看定义了系统用户账户的文件，可以输入下面命令 下面列出 less 最常用的键盘命令 第四章 操作文件与目录 由于 shell 需要经常使用文件名，因此它提供了一些特殊字符来帮助你快速指定一组文件名。这些特殊字符称为通配符，通配符允许用户依据字符模式选择文件名 mkdir 命令 mkdir 命令用于创建目录，格式如下，mkdir directory...，注意参数后面带有3个点号，表示该参数可以重复，例如 mkdir dir1 dir2 dir3 cp 命令 cp 命令用于复制和目录，格式如下，cp item1 item2 或 cp item... directory mv 命令 mv 命令可以执行文件移动和重命名操作，在这两种情况下，完成操作之后，原来的文件名都将不存在，格式类似于 cp，mv item1 item2 或 mv item... directory rm 命令 rm 用来删除文件和目录，格式如下，rm item...，类 UNIX 操作系统并不包含还原删除操作的命令，一旦使用 rm 命令就彻底删除了。所以在 rm 命令与通配符一起使用时需要特别小心 举个例子，我们要删除 html 文件，使用如下的命令 rm *.html 但是如果错误多输入了一个空格在 * 和 .html 之间，那么 rm 命令将会删除目录中所有文件，并提示说明目录中没有叫做 .html 的文件。对于这个问题，我们有一种常用的解决办法，先用 ls 命令列出所有的 .html 文件，然后再换为 rm 开头 下面列出了 rm 命令的一些实例，以及常用选项 ln 命令 ln 命令可用来穿件硬链接或者是符号链接，ln file link 用来创建硬链接，ln -s item link 用来创建符号链接，这里的 item 可以是文件也可以是目录 当创建一个硬链接的时候，也为这个文件创建了一个额外的目录条目，硬链接有两条重要的局限性： 硬链接不能引用与该链接不在同一磁盘分区的文件 硬链接无法引用目录 符号链接则是通过创建一个特殊类型的文件来起作用，该文件包含了指向引用文件或目录的文本指针 命令的使用 一条命令不外乎以下 4 中情况： 可执行程度：可执行程序就像在 /usr/bin 目录里看到的所有文件一样，在该程序类别中，程序可以编译为二进制文件 shell 内置命令：bash 支持粗多在内部称之为 shell builtin 的内置命令。例如 cd 命令就是一个 shell 内置命令 shell 函数：shell 函数是合并到环境变量中的小型 shell 脚本 alias 命令：我们可以在其他命令的基础上定义自己的命令 type 命令 type 命令是一个 shell 内置命令，可根据指定的命令名显示 shell 将执行的命令类型 which 命令 有时候，系统中可能会安装了一个可执行程序的多个版本，使用 which 命令可以确定一个给定可执行文件的准确位置 which 命令只适用于可执行程序，而不适用于内置命令和命令别名。试图对 shell 内置命令（例如，cd）使用 which 命令，要么没有响应，要么得到一条错误信息 help 命令 bash 为每一个 shell 内置命令提供了一个内置的帮助工具，输入 help，然后输入 shell 内置命令的名称就可以使用该帮助工具，例如 很多可执行程序也都支持 --help 选项，--help 选项描述了命令支持的语法和选项，例如 man 命令 大多数供命令行使用的可执行文件都提供一个称之为 manual 或者 man page 的正式文档，该文档可以用一种称为 man 的特殊分页程序来查看，格式如下。手册文档在格式上会有所不同，但是通常都包括标题、命令句法的摘要、命令用途的描述、命令选项列表以及每个命令选项的描述 在大多数 Linux 系统中，man 命令会调用 less 命令来显示手册文档，所以，当显示手册文档时，所有的 less 命令都能奏效 man 命令显示的文档手册被分为多个部分，它不仅包括用户命令，也包括系统管理命令、程序接口、文件格式等。如果没有指明部分编号，通常我们会获得第一次匹配的实例，一般情况下，我们按照如下格式使用 man 命令，man section search_term，例如，man 5 passwd 将会显示文件 /etc/passwd 的文件格式描述手册 apropos 命令 apropos 命令在一些特定的包含系统命令的简短描述的数据库文件里查找关键字，然后把结果送到标准输出。如果你不知道完成某个特定任务所需要命令的名称，可以使用一个关键字通过 apropos 来搜索它。该实用程序可以搜索关键字并且显示所有包含匹配项的 man 页面的简短描述。另外，使用 man 实用程序和 -k（关键字）选项，可以得到和用 apropos实用程序相同的结果（实际上是相同的命令） whatis 命令 whatis 程序显示匹配句意关键字的手册页的名字和一行描述 info 命令 GUN 项目提供了 info 页面来代替手册文档，info 页面可以通过 info 阅读器来显示，info 页面使用超链接，这与网页结构很相似。下面是 info ls 的现实 info 程序读取 info 文件，info 文件包含的超链接可以实现节点间的跳转，通过前置星号可以识别超链接，将光标放在超链接上并按回车键，可以激活它。例如下面光标选中的即使一个超链接 可以通过输入 info 以及程序名来调用 info，下表列出了显示 info 页面时，用于控制阅读器的命令 到目前为止，我们讨论的大部分命令行程序都是 GNU 项目 coreutils 包的一部分，输入命令 info coreutils 我们将会看到一个菜单页面，该菜单页面包含了 coreutils 包提供的每个程序的文档的超链接 alias 命令 在讲解 alias 命令之前，我们先展示一个命令行的小技巧——通过使用分号来分隔多条命令，就可以将多条命令输入在一行中，其工作方式如下，cmd1; cmd2; ...，例如 现在，我们可以通过使用 alias 命令将以上命令整合成一条新的命令。首先，我们将新命令叫做 foo，不过在整合之前，我们还是最好检查一下名称 foo 是否已经被使用过了，为此我们需要使用 type 命令 接着，我们调用下面命令来创建 foo 命令，注意这个命令的结构 alias name='string' 试试我们新定义好的命令 也可以再次使用 type 命令来查看别名 要删除别名，可以使用 unalias 命令，如下所示 我们回去有意避免已经存在的名称来给我们的命令命名，但有时也会故意这么去做。这样做的目的是，为每一个经常调用的名利女添加一个普遍会用到的选项。例如，前面讲到的为 ls 命令添加别名，以添加颜色支持 要查看环境中定义的所有别名，可一般使用不带任何参数的 alias 命令，如下所示 我们需要注意，这里定义的命令都会随着 shell 会话的结束而消失，在后面我们会学习如何向文件中添加别名。每一次登录系统时，这些文件都会建立系统环境 第六章 重定向 与UNIX 一切都是文件的思想一致，类似 ls 的程序实际上把它们的运行结果发送到了一个称为标准输出（stdout）的特殊文件中，它们的状态信息则发送到了另一个称为标准错误（stderr）的文件中。默认情况下，标准输出和标准错误都将被链接到屏幕上，并且不会被保存在磁盘文件中 另外，许多程序从一个称为标准输入（stdin）的设备来得到输入，默认情况下，标准输入连接到键盘 I/O 重定向功能可以改变输出内容发送的目的地，也可以改变输入内容的来源地 标准输出重定向 I/O 重定向功能可以重新定义标准输出内容发送到哪里，使用重定向操作符 >，后面接文件名，就可以把标准输出重定向到另一个文件中。比如，我们可以通过下面的形式把 ls 命令 的输出保到 ls-output.txt 通过 less 命令即可查看 ls-output.txt 这里我们重定向了标准输出，但是如果我们对一个不存在的目录执行 ls 命令重定向到 ls-output.txt 会怎么样呢？终端打印错误信息，原因是我们只重定向了标准输出，而没有重定向标准错误。于此同时我们还能发现，这时候 ls-output.txt 内容是空的，因为重定向操作重写了这个文件，因为没有任何标准输出所以文件是空的 这里我们就对重定向有一个妙用，通过下面命令我们可以创建一个新的空的文件，或者删除文件之前的内容 如果我们希望的操作是在文件末尾追加呢？可以通过 >> 重定向符实现 标准错误重定向 介绍一个叫做文件描述符的内容，一个程序可以把生成的输出内容发送到任意的文件流中，而在 shell 内部用文件描述符 0、1、2 来表示标准输入文件、标准输出文件和标准错误文件。那么标准错误重定向就可以通过下面的命令实现 许多情况下，我们希望把一个命令的所有输出内容都放在同一个文件中，于是我们必须同时重定向标准输出和标准错误，有两种方法可以进行 第一种首先重定向标准输出到 ls-output.txt 文件中，然后使用标记符 2>&1 把文件描述符 2 重定向到文件描述符 1，这些重定向操作的顺序非常重要，标准错误的重定向通常发生在标准输出重定向操作之后，否则它将不起作用。也就是说，对于下面的命令，如果是这样 2>&1 > ls-output.txt 那么标准错误还是会重定向到屏幕上 第二种方法是使用 &> 这在许多的新版本中都支持，第一种方法在一些旧版的 shell 中使用 有些时候，我们需要隐藏一些输出的信息，这时候系统提供了一种方法，通过把输出重定向到一个称为 /dev/null 的特殊文件中，这个文件是一个称为位桶的系统设备，它接受输入但是不对输入进行任何处理。如下命令，可以用来隐藏一个命令的错误信息 cat 命令 cat 命令读取一个或多个文件，并把它们复制到标准输出文件中 cat 经常用来显示短的文本文件 由于 cat 可以接受多个文件作为输入参数，所以它也可以用来把文件链接到一起。假设，我们有一个很大的文件，在下载的时候分成了如下的子文件 可以使用命令让他们重新连接在一起 那么 cat 和标准输入有什么关系呢？我们只输入 cat 命令，没有任何参数，那么它将从标准输入读取内容，由于标准输入默认情况下是连接到键盘，cat 命令看似挂起什么也没做，实际是在等待键盘输入内容，这时我们输入一句话，按照 Ctrl+D（表示已经到达了标准输入的文件尾部 EOF），cat 将把标准输入内容复制到标准输出文件，所以会出现下面这样的情况 同时，我们也可以将标准输入的内容放到一个 txt 文件中，如下所示 使用重定向符 但是这么做没有太大意义，也可以直接使用 cat lazy_dog.txt 管道 命令从标准输入到读取数据，并将数据发送到标准输出的能力，是使用了名为管道的 shell 特性，使用管道操作符 | 可以把一个命令的标准输出传送到另一个命令的标准输入中，例如 过滤器 管道功能经常用来对数据执行复杂的操作，也可把多条命令合在一起构成一个管道。这种方式用到的命令通常被称为过滤器。过滤器接受输入，按照某种方式对输入进行改变，然后再输出它。例如， 可以将指定的两个目录的 ls 输出排列好成两个列表，每一个对应一个目录，如下所示 uniq 命令 上面通过一个例子展示了 sort 命令，与 sort 经常一起使用的有 uniq 命令，uniq 接受来自于标准输入或者一个单一文件名参数对应的已排好序的数据列表，默认情况下，该命令删除列表中的所有重复行。因此，在管道中添加 uniq 命令，可以确保所有的列表都没有重复行 如果反过来想要查看重复行的列表，可以在 uniq 命令后面添加 -d 选项 wc 命令 wc 命令都用来显示文件中包含的行数、字数和字节数，例如 结合上面的命令，如果我们要查看已经排好序的列表中的条目数，可以通过在 wc 命令后添加 -l 选项来限制值报告行数 grep 命令 grep 用来在文件中查找匹配文本，通常需要用到正则表达式，这里先不做详细介绍。grep 中有两个常用的选项：-i 选项可以在搜索时忽略大小写，-v 输出和模式不匹配的行 head/tail 命令 hea/tail 命令输出文件的前 10 行（最后 10 行），可以通过 -n 来调整输出的行数 当然，也可以将 tail 命令引用在管道中 tail 中有一个选型可以用来实时查看文件，该选项在观察正在被写入的日志文件的进展状态时非常有用。如下示例，使用 -f 选项可以持续监视这个文件，一旦添加了新的行，将会立即显示在屏幕上，Ctrl-C 结束 tee 命令 tee 读取标准输入，再把读到的内容复制到标准输出（这里允许数据继续向下传递到管道中）和一个或多个文件中去。例如，在下面的命令中，我们在使用 grep 搜索文件之前，先使用 tee 命令来获取整个目录列表输出到 ls.txt 文件中去 第七章 透过 shell 看世界 扩展 有了扩展的功能，在输入内容后，这些内容将在 shell 对其执行之前被扩展为其他内容，我们以 echo 命令为例子，echo 命令将文本参数内容打印到标准输出 我们发现如果参数变成 ， 那么 echo 输出的并不是 ，这是为什么呢？* 意味着匹配文件名中的任意字符，shell 在执行 echo 命令前把 * 字符扩展为当前工作目录下的所有文件名，所以在按下 enter 键之后，shell 会在执行命令前自动扩展命令行中所有符合条件的字符 路径名扩展 通过使用通配符来实现扩展的机制称为路径名扩展，例如 我们说过以一个 . 点字符开头的文件时隐藏文件，如果我们想要显示这些文件呢？可能你会想到下面的命令，但遗憾的是我们发现，当前工作目录和当前目录的父目录也被匹配到了 如果只想匹配那些隐藏文件，可以尝试下面一种更精确的命令 波浪线扩展 波浪线 ~ 如果用在一个单词的开头，那么它将被扩展为指定用户的主目录名，如果没有指定用户名，则扩展为当前用户的主目录 添加新用户 如果我们没有用户 foo，那么 echo ~foo 的输出将是 ~foo，所以为了得到和书中一样的结果，我们尝试添加新的用户 foo，具体操作可以参考 Linux系统使用添加新用户后，没有用户目录（没有home）解决办法 算术扩展 shell 支持通过扩展来运行算是表达式，具体格式是 $((expression))，暂时的算术扩展只支持整数运算，没有小数 花括号扩展 有了花括号扩展，可以按照花括号里面的模式创建多种文本字符串 花括号表单时本身可以包含一系列都好分隔的字符串，也可以包含一系列整数或者单个字符 同事，花括号扩展还支持嵌套 那么花括号扩展一般应用在那些地方呢？最普遍的是创建一系列的文件或者目录 参数扩展 我们这里先简单地介绍一下参数扩展，参数扩展在 shell 脚本中比直接用在命令行中更为有用。它的许多特性与系统存储的变量以及给每个变量命名的性能有关。我们举个例子，USER 变量包含你的用户名，为了触发参数扩展，并显示出 USER 的内容，可以进行如下的操作 想要查看可用的变量列表，可以试试下面的命令 对于其他的扩展类型来说，如果错误输入了一个模式，就不会发生扩展，这时 echo 命令将只是显示这些错误输入的模式信息。而对于参数扩展来说，如果变量名拼写错误，仍然会进行扩展，只不过结果是的输出一个空字符串而已 命令替换 命令替换可以把一个命令的输出作为一个扩展模式使用，如下所示 下面这个命令把 which cp 命令的运行结果作为 ls 命令的一个参数，因此我们无需知道 cp 程序所在的完整路径就能获得 cp 程序对应的列表 引用 如果把文本放在双引号中，那么 shell 使用的所有特殊字符都将失去他们的特殊含义，而被看成普通字符。换句话说，这意味着单词分割、路径名扩展、波浪线扩展和花括号扩展都将失效，但是参数扩展、算数扩展和命令替换仍然生效 于是对于一个带空格的文件名，我们可以这么处理 默认情况下，单词分割会把空格、制表符和换行符当做单词间的界定符。而在加上双引号后，这些都将被视为文本的一部分。例如下面的命令，第一个例子中没有加上双引号导致命令行被识别为命令后面跟着 38 个参数，而在第二个例子中加入双引号后，命令行被识别为命令后只跟着一个参数，这个参数包含嵌入空格和换行符 如果我们希望抑制所有的扩展就使用单引号 转义字符 转移字符就不多做介绍，有时也用来消除文件名中某个字符的特殊含义，比如，\"$\"、\"!\"、\"&\" 等 第八章 高级键盘技巧 历史命令 我们可通过下面命令查看历史记录 也可以通过历史记录了扩展来执行对应行号的历史命令，例如下面命令 bash 将 ！88 扩展成 ls -l /usr/bin > ls-output.txt 下表给出了历史记录的扩展命令 bash 也支持以递增的方式搜索历史记录，在空白命令行处按下 Ctrl+R 将出现下面的提示符，这时候输入要查找的内容就可以开始递增式的搜索。当找到要查找的内容时，按 Enter 键可以直接执行次命令，也可以按下 Ctrl+J 将搜索内容复制到当前命令行 第九章 权限 id 命令 使用 id 命令可以获得用户身份标识的相关信息，在创建用户账户的时候，用户将分配一个称为用户 ID 的号码（uid） ，同时用户被分配一个有效组 ID（gid） 对于每一个用户账户，文件 /etc/passwd 中都定义了对用用户的用户名、uid、gid、账户的真实姓名、主目录以及登录shell 信息。用户组定义在文件 /etc/group 中。在创建用户账户和群组 时，这些文件随着文件 /etc/shadow 的变动而变动，文件 /etc/shadow 中保存了用户的密码信息 读取、写入和执行 前面提过在使用 ls -l 显示时，开头的字段表示文件属性，具体如下所示 chomod 命令 我们可以使用 chmod 命令来更改文件或者目录的权限。需要注意的是只有文件所有者和超级用户才可以更改文件或者目录的模式。chmod 命令支持两种不容的改变文件模式的方法——八进制数字表示法和符号表示法 八进制表示法就是用八进制数字来设置所期望的权限模式。因为每个八进制数字对应着 3 个二进制数字 下面通过八进制表示法改变 foo.txt 的权限 符号表示法分为三部分：更改谁的权限、执行哪种操作、设置哪种权限。对于更改谁的权限，我们可以通过字符 u、g、o 和 a 来指定要影响的对象 如果没有指定字符，则假定使用 all 操作符 + 表示添加一种权限，- 表示删除一种权限，= 表示只有指定的权限可用，其他所有的权限都被删除，例如 umask 命令 umask 命令控制着创建文件时指定给文件的默认权限。它使用八进制表示法来表示从文件模式属性中删除一个位掩码。这句话看起来很绕口，我们举个例子，在下表中，我们通过 0002 使得其他所有用户只有读权限 如果我们希望其他所有用户获得写权限呢？可以执行如下的命令 记得每次操作完之后清理文件，并把掩码值还原到默认值 在大多数情况下，并不需要修改掩码值。然后而一些高安全级别的环境下，则需要自己控制掩码值 我们通常看到的八进制权限掩码使用三位数字来表示，但是确切地说，有四位数字。因为除了基本的读写执行权限外，还有一些其他的较少用到的权限设置 setuid 位（八进制表示为 4000），当它应用到一个可执行文件时，有效用户 ID 将从实际用户 ID 设置成该程序所有者的 ID。于是，当普通用户运行一个具有 setuid root 属性的程序时，该程序将以超级用户的权限来执行。这使得该程序可以访问一些普通用户通常禁止访问的文件和目录。很明显，这会带来安全方面的问题，所以允许设置 setuid 位的程序个数必须控制在绝对小的范围内 setgid 位（八进制表示为 2000），类似于 setuid，它会把有效组 ID从该用户的实际组 ID 更改为该文件所有者的组 ID。如果对一个目录设置 setgid 位，那么在该目录下新创建的文件将由该目录所在组所有，而不是由文件创建者所在组所有。当一个公共组下的成员需要访问共享目录下的所有文件时，设置 setgid 位很有用，并不需要关注文件所有者所在的有效组 sticky 位，标记一个可执行文件为不可交换的，在 Linux 中，会忽略文件的 sticky 位，但是如果对一个目录设置 sticky 位，那么将能阻止用户删除或者重命名文件，除非用户是这个目录的所有者、文件所有者或是超级用户。它常用来控制对共享目录的（例如，/tmp）的访问 更改身份 切换用户的命令为： su username 从普通用户切换到 root 用户，还可以使用命令： sudo su / su 如果包含 -l 选项，那么该指定用户的运行环境将被加载，而且其工作目录也将更改为该指定用户的主目录 在使用结束时，输入下面命令就可以返回到之前的 shell 环境 exit 我们也可以使用 su 命令执行单个命令，而不需要开启一个新的交互式命令界面 su -c 'command' 使用这种格式，单个命令被传递到一个新的 shell 环境下进行执行。这里需要单引号把命令行引起来 sudo 命令在很多方面都类似于 su 命令，但是它还是另外有一些重要的功能。管理者可以通过配置 sudo 命令，使系统以一种可控的方式，允许一个普通用户以超级用户的身份执行命令。在使用 sudo 命令时，并不需要输入超级用户的密码，而只需要用户输入自己的密码来进行认证 su 命令和 sudo 命令之间的另一个重要区别在于 sudo 命令并不需要启动一个新的 shell 环境，而且也不需要加载另一个用户的运行环境 chown 命令 chown 命令用来更改文件或目录的所有者和所属群组。使用这个命令需要超级用户的权限。chown 命令的语法格式如下： chown [ower][:[group]] file ... 我们假设两个用户，超级用户 janet 和普通用户 tony，janet 从她的主目录复制一个文件到 tony 的主目录中，同时希望 tony 能够编辑该文件，操作如下 值得注意的是，在 janet 第一次使用了 sudo 命令之后，系统为什么没有提示她在后面命令输入密码呢？这是因为，在大多数的环境配置下，sudo 会短暂的信任用户几分钟 第十章 进程 ps 命令 ps 命令有很多选项，其中最简单的使用格式如下所示。这个例子的输出结果列出了两个进程，分别对应 bash 命令和 ps 命令。TTY 代表额进程的 控制终端，TIME 表示进程消耗的 CPU 总时间 我们可以在 ps 后面加上 x 选项，这将告知 ps 命令显示所有的进程而不需要关注它们是由哪个中断所控制。TTY 列中出现的问号表示没有终端控制。STAT 显示进程的当前状态，具体解释在表 10-1 中列出 另一个常用的选项是 aux，该选项会显示属于每个用户的进程信息，使用这些选项时不带签字连字符将是的命令以 BSD 模式运行 top 命令 ps 提供的只是命令被执行时刻机器状态的一个快照，要查看机器运行情况的动态视图，我们可以使用 top 命令。top 程序加你个按照进程活动的顺序，一列表的形式持续更新显示系统进程的当前信息。top 命令显示的内容包括两个部分，顶部显示的是系统总体状态信息 下面显示的是一张按 CPU 活动时间排序的进程情况表的头部，我们对其中的各字段进行解读。至于进程显示的字段常用的已经在上面介绍了，这里就不再重复 后台运行/回到前台 我们先介绍一个自带的小程序—— xlogo，简单来说就是运行后会出现一个带 logo 的课缩放窗口。我们可以通过在 xlogo 后加上 & 符号将其推到后台运行 我们可以看到 shell 返回了一条信息，这条信息是 shell 的一个称为作业控制的特性表现，它告诉我们已经启动的作业编号为 1，其对应的 PID 是 28236 shell 的作业控制特性也提供了一种方式来查看从该终端启动的所有作业，使用 jobs 命令可以得到如下列表信息 反过来，我们可以通过在 fg 命令后面加上百分比符号和作业编号将后台任务返回到前台 暂停进程 如果我们只是想暂停前台程序的话，按下 Ctrl+Z 即可，这时候我们可以选择使用 fg 命令让进程恢复前台运行，也可以通过 bg 命令让进程移到后台运行 信号 我们可以通过 kill 命令来杀死一个进程，准确来说，kill 并不是杀死进程，而是给进程发送信号。例如，在按下 Ctrl+C 的情况下，终端将发送一个称为 INT (Interrupt) 的信号；在按下 Ctrl+Z 的情况下，终端将发送一个称为 TSTP (Terminal Stop) 的信号 程序侦听信号，而且在接收到信号的时候按照他们的指示进行操作。程序可以侦听信号并且可以按照信号指示操作的这一特性，使得程序在接收到终止信号的时候可以保存当前正在进行的工作 如果在 kill 命令行中没有指定信号，那么默认发送 TERM (Terminate) 信号 我们可以通过信号编号来指定 kill 发送的信号 也可以通过信号名来指定信号，其中包含带有 SIG 前缀的信号名 下面列出了其他一些经常被系统使用的信号 killall 命令 通过 killall 命令可以给指定程序或者执行用户名的多个进行发送信号 第十一章 环境 printenv 命令 printenv 命令显示指定的环境变量的值，如果没有指定变量，则打印出所有变量的名称和值。 通过 printenv 命令，我们可以查看到当前的 shell，和 shell 的环境变量，和其它一些常见的配置 set 命令 set 命令用于设置 shell，我们也可以通过下面的命令显示环境变量 如果要查看单个变量的值，我们也可以使用 echo 命令，如下所示 环境是如何建立的 用户登录系统后，bash 程序就会启动并读取体一系列称为启动文件的配置脚本，这些脚本定义了所有用户共享的默认环境。接下来，bash 会读取更多存储在主目录下的用于定义个人环境的启动文件 login 和 non-login shell shell 会话存在两种类型，分别为 login shell 和 non-login shell，login shell 会话会提示用户输入用户名和密码，而一般从 GUI 中启动的终端都是一个典型的 non-login 会话 下面两表列出了两种 shell 的启动文件，一般 non-login shell 除了启动下表的文件还会继承父类进程的环境，父类进程通常是一个 login shell 要查看本机系统有哪些启动文件，需要注意这些文件大多数以 \".\" 开头，所以在使用 ls 命令时要加上 -a 选项 启动文件 下图是一个典型的 .bash_profile 内容 关键内容是 if 里面包裹的语句，其大致意思可以理解为 if the file \"~/.bashrc\" exists, then read the \"~/.bashrc\" file 此外，在上述文件中还有一个重要的元素—— PATH 变量 实际上，当用户输入命令 ls，shell 并不会搜索整个系统来寻找 /bin/ls，而是搜索 PATH 变量中存储的目录列表。PATH 变量通过由启动文件 /etc/profile 中的一段代码设定 PATH=$PATH:$HOME/bin 这段代码将 $HOME/bin 添加到了 PATH 值的尾部，当系统需要检索用户输入的命令时，$HOME/bin 这个路径就会处于被搜索的路径列表中 最后一行代码将告诉 shell 的子进程使用 PATH 变量的内容 export PATH 使用文本编辑器 所有的文本编辑器都可以通过在命令行中输入编辑器名称和需编辑的文件名称的方式启动。如果输入的文件不存在，编辑器会认为用户想要穿件一个新的文件，例如下面命令 图形界面编辑器都非常易于理解，不做赘述。我们下面通过对 .bashrc 文件进行编辑来讲解 nano，在此之前我们先备份一下 .bashrc 文件，扩展名 .bak，.sav，.old 和 .orig 是常用的标识备份文件的方法 备份完文件之后，就可以启动文本编辑器了 屏幕显示内容分为三部分：顶端的标题、中间的可编辑文本和底部的命令菜单。我们在 .bashrc 文件的最后加上如下的代码 解释如下所示 一种激活修改的方式是重启 shell，当然也可以使用下面的命令强制命令 bash 重新读取 .bashrc 文件 现在我们就可以试试 ll 命令 第十二章 VI 简介 输入 :q 可以退出 vi，当因为一些原因 vi 不能够退出时可以通过在命令后添加感叹号强制退出 vi，:q! 如果用户不能确定 vi 所处的状态，可以按两次 Esc 返回初始状态 通过 vi foo.txt 可以创建新的文件，vi 启动后进入的是命令模式，此时几乎键盘上的每一个按键都代表了一条命令 如果想要向文件中添加一些内容的话可以按下 I 键，编辑完之后按 Esc 返回命令模式，输入 :w 可以将文件写入硬盘，文件写入硬盘驱动之后，用户会在屏幕底部得到一条确认消息 下面列出了 vi 中一些移动光标的功能键 许多 vi 命令的前面都就可以有一个数字，前缀数字控制命令执行的次数，比如 5j 可以使得光标下移 5 行 文本插入 如果想在 foo.txt 追加一些内容，可以使用 A (大写) 命令——使光标移动到行末并进入插入模式 有些时候我们会在两行之间插入一些内容，这时可以使用下面两个命令 文本删除 对应有文本插入，就有文本删除，下面列举了 x 命令和 d 命令的使用 粘贴复制 d 命令不只是删除文本，而是在剪切文本，用户每次使用 d 命令之后，都会复制删除的内容到缓存，然后用户可以使用 p 命令将缓存中的内容粘贴到光标之后或只用 P 命令将内容复制到光标之前 y 命令也会复制文本 合并行 vi 在行的概念上非常严格，通常来说，将光标移动到行的末端并删除行的末尾字符并不能将此行与下一行合并。因此，vi 专门提供了 J (大写) 命令 查找替换 f 命令在行内进行搜索，并将光标移至搜索到的下一个指定字符。比如，fa 就会将光标移动到本行下一处出现字符 a 的地方。在执行过一次行内搜索后，输入分号可以使 vi 重复上一次的搜索 如果想要搜索整个文件的话，可以使用 / 命令，输入命令后，屏幕底部会出现一个 / 符号，接下来输入需要搜索的单词或短语即可 vi 使用 ex 命令来执行几行之内或者整个文件中的搜索和替换操作，输入以下命令可将文件中的 Line 替换为 line :%s/Line/line/g 该命令的解析如下 如果在上面命令的末尾添加 c，则命令在每次替换之前都会请求用户确认，即 :%s/Line/line/gc 每次替换前，vi 都会停下来询问用户是否确认执行替换，如下所示 编辑多个文件 如果需要同时编辑多个文件，可以通过在命令行具体制定多个文件的方式使 vi 打开多个文件 vi file1 file2 file3 使用以下 ex 命令可以从一个文件切换到下一个文件 :n 切换回上一个文件 :N 当用户从一个文件切换到另一个的时候，vi 要求用户必须先保存对当前文件做出的修改，若要放弃对文件的修改并使 vi 强制切换到另一个文件，可在命令后面添加 ！ 除了上面描述的切换方法，vim（和一些版本的 vi）还提供了一些 ex 命令让用户可以更轻松地编辑多个文本，例如，用户可以使用 :buffers 命令来查看正在编辑的文件列表 输入 :buffer 2 可以从文件 1 切换到文件 2 当然在我们已经打开一个文件的时候也可以使用 :e file2 命令载入另一个文件 文件之间的内容复制 在编辑多个文件时，我们有时会需要在不同文件之间来回复制内容，使用之前使用过的复制和粘贴命令即可，如切换到文件 1 文件 1 的内容如下所示，使用 yy 命令复制第一行 然后切回文件 2 文件 2 内容如下所示，在第一行使用 p 命令即可将前面文件 1 中复制的内容粘贴到文件 2 中 插入整个文件 我们还可以将一个文件完全插入到正在编辑的文件中，只要使用下面命令即可 :r foo.txt 命令 :r 将指定的文件内容插入到光标位置 保存工作 在命令模式下输入 ZZ 将保存当前文档并退出 vi，同样也可以使用 ex 命令 :wq 保存并退出 vi 当 :w 命令指定一个随意的文件名时，命令的功能将类似于另存为，如下面命令将当前编辑的文件另存为 foo1.txt :w foo1.txt 撤销和恢复撤销 关于 u 和 Ctrl + R 都比较好理解，就会一个撤销，一个恢复撤销 我们举个例子讲一下 U（大写），它的作用是撤销或恢复撤销对光标所在行文本所做的全部操作，比如我们有一个文本 demo.txt http://c.biancheng.net http://c.biancheng.net http://c.biancheng.net # 在第 3 行做如下修改 http://c.biancheng.net http://c.biancheng.net Linux教程 http://c.biancheng.net/linux_tutorial/ 在修改之后的第一次键入 U，可以看到，第三行的两处修改被撤销了 http://c.biancheng.net http://c.biancheng.net http://c.biancheng.net 如果，此时再次键入 U，那么又会恢复之前的撤销 http://c.biancheng.net http://c.biancheng.net Linux教程 http://c.biancheng.net/linux_tutorial/ 第十三章 定制提示符 系统默认的提示符看起来如下所示 提示符中包含了用户名、主机名和当前的工作目录。提示符由名为 PS1（prompt string 1）的环境变量定义，通过 echo 命令可以查看 PS1 的值 PS1 值的解释如下 通过这个特殊字符列表，我们可以更改提示符来查看效果，具体就不介绍了没有太大意义 第十四章 软件包管理 这一章没啥介绍的，都是一些关于软件包安装、卸载和更新的操作，用到的时候百度吧 第十五章 存储介质 mount 命令 mount 命令用于文件系统挂载，不带任何参数输入该命令将会调出目前已经挂载的文件系统列表，列表的格式是：device on mont_point type filesystem_type (options)，以第一行为例，dev/sda2 设备挂载在根目录下，属于 ext3 类型，可读写（后面 的参数选项是 rw） 我们以 CD-ROM 来讲解一下挂载和卸载，下图是插入 CD-ROM 之前的系统信息 插入 CD-ROM 后会自动进行挂载，如下面最后一行所示 获取 CD-ROM 的设备名之后，边可以卸载该设备，然后将其挂载到文件系统树上的另外一个结点上。我们先通过超级用户来卸载光盘 接下来，我们为光盘穿件一个新的挂载点，如果挂载在非空目录上会导致该目录下原有内容不可见，所以我们创建了一个新的目录 然后，将 CD 光盘挂载在了新创建的结点上，使用 -t 选项指定文件系统类型，之后就可以通过新建的挂载结点访问 CD 光盘的内容 如果此时视图卸载 CD 光盘会出现下面的问题，因为设备正在被某人或是某程序使用时是不能卸载的，而此时的工作目录还是在 cdrom，所以导致了 “设备繁忙” 的错误警告 于是，我们将工作目录改到挂载结点意外的地方就可以解除占用了 我们稍微提一句，卸载为什么会比较重要。缓存的概念我们都懂，卸载设备能够确保缓存中的所有剩余数据全部写入设备中，从而设备能被安全移除。如果设备实现没有卸载就被移除，那么缓存中就可能仍有剩余数据。有些情况下，这些未传输完的数据可能包含重要的目录更新信息，而这些信息会导致文件系统损坏，此时传输完的文件可能在下次就不能被正确读取 对于这一章剩下的内容，我们就直接跳过吧，用到的时候自行百度，这里不好进行实验 第十六章 网络 ping 命令 ping 命令会向指定的网络主机发送特殊网络数据包，多数网络设备收到该数据包后会做出回应，通过此法即可验证网络连接是否正常 ping www.baidu.com traceroute 命令 traceroute 会显示文件通过网络从本地系统传输到指定主机过程中所有停靠点的列表 由该列表可知，从测试系统到 http://www.slashdot.org/ 网站的连接需要经过 16 个路由器 netstat 命令 nestate 可以用于查看不同的网络设置及数据，具体用到时再做说明 ftp 命令 下面是一个典型的 ftp 会话，其功能是从匿名 FTP 服务器 fileserver 上的 /pub/cd_image/Ubuntu-8.04 目录中下载 Ubuntu ISO 映像文件 wget 命令 wget 是另一个用于文件下载的命令行程序，至于用法很常见就不再赘述 ssh 命令 为了解决明文传送的问题，一个叫做 Secure Shell（SSH）的新协议应运而生。SSH 协议解决了与远程主机进行安全通信的两个基本问题： 该协议能验证远程主机的身份时候真实，从而避免中间人攻击 该协议将本机与远程主机之间的通信内容全部加密 SSH 协议包括两个部分：一个是运行在远程主机上的 SSH 服务端，用来监听 TCP 端口 22 上可能过来的连接请求；另一个是本地系统上的 SSH 客户端，用来与远程服务器进行通信 例如，下面通过 ssh 来连接远程主机 remote-sys 第一次尝试连接的时候，由于 ssh 程序从来没有接触过此远程主机，所以会跳出一条 warning 输入密码后就可以登录到远程主机了，远程 shell 对话将一直开启着，直到用户在该对话框中输入 exit 命令断开连接 如果希望登录远程主机上的其他用户的话，可以尝试下面命令 scp 命令 从本地复制到远程 scp local_file remote_username@remote_ip:remote_folder scp local_file remote_username@remote_ip:remote_file # 复制目录 scp -r local_folder remote_username@remote_ip:remote_folder 从远程复制到本地将顺序反一反就好啦 sftp 命令 sftp 和 ftp 极为相似，只是 sftp 是用 SSH 加密隧道传输信息而不是以明文方式传输。sftp 相比传统的 ftp 而言，还有一个优点 ，就是它并不需要远程主机上运行 FTP 服务器，仅仅需要 SSH 服务器 第十七章 文件搜索 locate 命令 locate 命令通过快速搜索数据库寻找路径名与给定子字符串相匹配的文件，例如下面命令 当然也可以将 locate 命令和 grep 结合 find 命令 locate 查找文件仅仅是依据文件名，而 find 则是依据文件的各种属性在既定的目录及其子目录里查找 下面就是用 find 命令列出当前系统主目录下的文件列表清单 find ~ 同样还可以用 wc 计算 find 搜索到的文件的总量 find ~ | wc -l 假定我们想要查找的是目录文件，则可以添加 -type 选项来达到此目的，-type d 限制搜索Wie目录，-type f 则表示只对普通文件进行搜索 find ~ -type d | wc -l 下表列出了 -type 支持的文件类型 同样，我们也可以对文件的大小做出限制，例如 find ~ -type f -name \"*.JPG\" -size +1M | wc -l +1M 表示文件大小大于 1M find 命令还支持逻辑操作符，如我们想查找那些访问权限既不是 0600 也不是 0700 的子目录 对搜索到的文件进行操作，既可以用诸多现成的预定义动作指令，也可以使用用户自定义的动作 在没有指定其他操作的情况下，-print 操作是默认的，也就是说 find ~ 其实等价于 find ~ -print 我们考虑䦹下面一个例子来说明逻辑运算是如何影响 find 的 action 操作 find ~ -type f -name '*.BAK' -print 这条命令的逻辑应该是这样的，找到一个普通文件，这个普通文件是 .BAK 文件，最后打印出来 find ~ -type f -and -name '*.BAK' -and -print 如果命令变成下面的形式，那么搜索到的每个文件都先打印出来，然后再去匹配 -type 和 -name find ~ -print -and -type f -and -name '*.BAK' 除了已有的预定义操作命令，同样也可以任意调用用户想要执行的操作命令，一般是是使用 -exec，例如用 -exec 完成 -delete 的操作 -exec rm '{}' ';' 花括号代表当前路径，分好作为必须的分隔符表示命令结束。由于花括号和分好在 shell 环境下有特殊的含义，所以在输入命令行时，要将它们用引号引起来，或者用转义符隔开 如果希望交互式地执行用户自定义的操作，使用 -ok 替代 -exec 即可 通过将命令行末尾的分号给为加号，便可将 find 命令所搜索到的匹配结果作为指定命令的输入，从而一次完成对所有文件的操作。例如，下面命令每次找到匹配文件后就执行一次 ls 命令 而将分号改为加号的话，我们得到相同的结果，但是系统整体只执行一次 ls 命令 同样我们可以使用 xargs 命令获得相同的效果，xargs 处理标准输入信息并将其转变为某指定命令的输入参数列表。例如，在下面命令中，find 执行的结果将直接作为 xargs 的输入，xargs 反过来将其转换成了 ls 命令的输入参数列表，最后执行 ls 操作 批量查找空文件及空文件夹并删除 查找空文件 find . -name \"*\" -type f -size 0c 查找所有的空文件夹 find -type d -empty 查询所有/root/下的空文件夹 find /root -type d -empty 删除所有空文件夹 find -type d -empty | xargs rm -rf 查找指定名称的文件并删除 find ./ -name 'qipa250.log' -exec rm -rf {} \\; 这里做一个简单的补充： exec 是一个后续命令，{} 内的内容代表前面查找出来的文件 删除前有提示 find ./ -name 'qipa250.log' -ok rm -rf {} \\; 查找指定特征的文件并删除，例如下面是删除所有的 log 日志文件 find ./ -name '*.log' -exec rm -rf {} \\; 第二十四章 编写第一个 shell 脚本 我们通过 shell 编写一个 hello world 程序，大多脚本的第一行都是以 #! /bin/bash 开头，#! 字符序列是一种称之为 shebang 的特殊结构，shebang 用来告知操作系统，执行后面的脚本应该使用的解释器的名字 接下来通过 chmod 命令使脚本可执行，有两种常见的权限设置：权限为 755 的脚本每个人都可以执行；权限为 700 的脚本，则只有脚本所有人才能执行 设置完权限之后，就可以执行脚本了 如果我们直接输入 hello_world 的话会提示没有该命令，为什么会这样呢？问题在于脚本的位置，我们之前讨论了 PATH 环境便令，以及它对系统搜索可执行程序方面的影响。如果没有显式指定路径，则系统在查找一个可执行程序时，需要搜索一系列目录。这就是当我们在命令行中输入 ls 时，系统知道要执行 /bin/ls 的原因。/bin 目录是系统会自动搜索的一个目录。目录列表存放在名为 PATH 的环境变量中。这个 PATH 变量包含一个由冒号分隔开的待搜索目录的列表。 如果脚本位于该列表中的任何一个目录中，问题就解决了。我们注意到列表中的第一个目录 /home/me/bin，大多数 Linux 发行版会配置 PATH 变量，使其包含用户主目录中的 bin 目录，允许用户执行他们自己的程序。如果我们创建了 bin 目录，并且将脚本放置在这个目录内，该脚本应该就会像其他命令一样直接运行 如果 PATH 环境变量中不包括 /home/me/bin 这个目录的话，可以通过在 .bashrc 文件中增加下面行 修改完毕之后，它会在每一个新的终端会话中生效。为了将这一修改应用到当前的终端会话，则必须让 shell 重新读取 .bashrc 文件 . 命令和 source 命令相同，是 shell 内置命令，用来读取一个指定的 shell 命令文件，并将其看作是像从键盘中输入的一样 第二十五章 启动一个项目 有如下两种不同方式可以通过 echo 命令输出文本，第一种方法比较笨拙，第二种在引入变量之后会显得比较间简洁一些 我们将脚本 sys_info_page 放在 ~/bin 目录下可以自己通过 sys_info_page 来显示脚本文本，如下所示 也可以将其修改为 .html 文件用 web 浏览器查看 除此之外还有称之为 here 脚本的第三种方法来输出文本，here 文档是 I/O 重定向的另外一种形式，我们在脚本中嵌入正文文本，然后将其输入到一个命令的标准输入中，如下所示 其中，command 表示接受标准输入的命令名，token 是用来指示嵌入文本结尾的字符串 于是，我们可以借助 here 文档按照如下方式显示上面的脚本 here 文档和 echo 命令类似，但是在默认情况下，here 文档内的单引号和双引号将失去他们在 shell 中的特殊含义，如下所示，这样就可以在 here 文档中随意嵌入引号，给我们的报告程序带来了很大的方便 第二十六章 自顶向下设计 shell 函数有两种语法形式， 下面脚本演示了 shell 函数的使用 在函数中，我们一般都会用到局部变量，在变量名前面添加一个单词 local 就可以定义局部变量，例如 于是，我们可以在上面 sys_info_page 脚本中添加几个函数来显示系统正常运行的时间、磁盘空间和用户空间 第二十七章 流控制：IF 分支语句 shell 中 if 语句的语法格式如下 我们用判断 x 是否等于 5 做一个例子 退出状态 命令执行完后会像操作系统发送一个值，这个值我们称之为退出状态。其实一个 0 ~ 255 的整数，0 表示执行成功，其他的数值表示执行失败，如下所示 文件表达式 下表列出了一些用于评估文件状态的表达式 可以通过下面的脚本来延时一下一些文件表达式的用法 关于这个脚本，有两个有趣的地方： 首先，要注意 $FILE 在表达式内是如何被引用的，尽管引号不是必须的，但是这可以防止参数为空的情况。如果 $FILE 的参数扩展产生一个空值，将会导致操作符被解释为非空的字符串，而不是操作符。用引号把参数括起来可以确保操作符后面总是跟着一个字符串，即使字符串是空 脚本末尾的 exit 命令接受一个单独的可选参数，它将成为脚本的退出状态。当不传递参数时，退出状态默认为 0。但是，当我们设定一个退出状态时，如果 $FILE 扩展为一个不存在的文件名时，就能使脚本提示失败了 类似地，通过在 return 命令中包含一个整数，shell 函数也可以返回一个退出状态，例如 字符串表达式 整数表达式 [[ ]] 和 =~ 我们下面介绍一下双中括号 [[ ]]， [[ ]]] 是 bash 程序语言的关键字，并不是一个命令。[[ ]] 结构比 [ ] 结构更加通用。在 [[ ]] 之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换 支持字符串的模式匹配，使用 =~ 操作符时甚至支持 shell 的正则表达式。字符串比较时可以把右边的作为一个模式，而不仅仅是一个字符串，比如 [[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号 使用 [[ ]] 条件判断结构，而不是 [ ]，能够防止脚本中的许多逻辑错误。比如，&&、||、 操作符能够正常存在于 [[ ]] 条件判断结构中，但是如果出现在 [ ] 结构中的话，会报错。比如可以直接使用 if [[ $a !=1 && $a != 2 ]]，如果不适用双括号，则为 if [ $a -ne 1] && [ $a != 2 ] 或者 if [ $a -ne 1 -a $a != 2 ] bash 把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码 [root@localhost ~]# [[ 2\\ [[ ]] 的另外一个特性是 == 操作符支持模式匹配，如下所示 最后，我们讲一下 =~ 操作符，对于 string1=~regex，如果 string1 与扩展的正则表达式 regrex 匹配，则返回 true (( )) 除了 [[ ]] 之外，bash 同样提供了 (( ))， (( )) 可用于整数扩展。这种扩展计算是整数型的计算，不支持浮点型。((exp)) 结 构扩展并计算一个算术表达式的值，如果表达式的结果为 0，那么返回的 退出状态码为 1，或者是 false，而一个非零值的表达式所返回的退出状态码 将为0，或者是 true。若是逻辑判断，表达式 exp 为真则为 1，假则为 0 只要括号中的运算符、表达式符合 C 语言运算规则，都可用在 $((exp)) 中，甚至是三目运算符。做不同进位（如二进制、八进制、十六进制）运算 时，输出结果全都自动转化成了十进制。如：echo $((16#5f)) 结果为 95（16进位转十进制） 单纯用 (( )) 也可重定义变量值，比如a=5; ((a++)) 可将 $a 重定义为 6 常用于算术运算比较，双括号中的变量可以不使用符号前缀。如可以直接使用if((i，如果不使用双括号，则为 if [ $i -lt 5 ] 组合表达式 组合表达式使用逻辑运算符将不同的表达式组合起来，下表列出了 test 和 [[ ]] 表示三种逻辑运算符的操作符 下面是一个 AND 运算的例子，这个脚本用来检测一个整数是否属于某个范围内 当然我们也可以使用 test 完成该功能 控制运算符 bash 还提供了两种可以执行分支的控制运算符，&& 和 ||，这和 [[ ]] 中的逻辑运算符很像，语法如下 command1 && command2 command1 || command2 对于 && 运算符来说，先执行 command1，只有在 command1 执行成功时，command2 才能够执行；对于 || 运算符来说，先执行 command1，只有在 command1 执行失败时，才执行 command2 第 28 章 读取键盘输入 read——从标准输入读取输入值 内嵌命令 read 的作用是读取一行标准输入，此命令课用于读取键盘输入值或应用于重定向读取文件中的一行，其语法结构如下所示 read [-options] [variable] 下表是 read 命令的选项，variable 是一到多个用于存放输入值的变量，若没有提供任何此类变量，则由 shell 变量 REPLY 来存储数据行 在下面脚本中，我们使用带有 -n 选项（时接下来的输出在下一行显示）的 echo 命令来输出一条提示符，然后使用 read 命令给 int 变量赋值，之后对输入的整数做一个简单的判断 #!/bin/bash # read-integer: evaluate the value of an integer echo -n \"please enter an integer ->\" read int if [[ \"$int\" =~ ^-?[0-9]+$ ]]; then if [ $int -eq 0 ]; then echo \"$int is zero.\" else if [ $int -lt 0 ]; then echo \"$int is negative.\" else echo \"$int is positive.\" fi if [ $((int %2)) -eq 0 ]; then echo \"$int is even.\" else echo \"$int is odd.\" fi fi else echo \"input calue is not an integer.\" >&2 exit 1 fi 运行的结果如下所示 在下面的脚本中，read 命令将输入值赋值给多个变量 #!/bin/bash # read-multiple: read multiple values from keyboard echo -n \"enter one or more values > \" read var1 var2 var3 var4 var5 echo \"var1 = '$var1'\" echo \"var2 = '$var2'\" echo \"var3 = '$var3'\" echo \"var4 = '$var4'\" echo \"var5 = '$var5'\" 下面运行结果中需要注意的当输入少于或多于 5 个值的时候 read 的运作方式 如果 read 命令之后没有变量，则会为所使用的输入分配一个 shell 变量：REPLY #!/bin/bash # read-single: read multiple values into default variable echo -n \"enter one or more values > \" read echo \"REPLY = '$REPLY'\" 运行结果如下所示 使用 IFS 间隔输入字段 shell 变量 IFS 控制了字符之间的分割符，例如，下面脚本，将 IFS 的值改为单个冒号从 /etc/passwd 文件读取对应用户的各字段信息 #!/bin/bash # read-ifs: read fields from a file FILE=/etc/passwd read -p \"enter a username > \" user_name file_info=$(grep \"^$user_name:\" $FILE) if [ -n \"$file_info\" ]; then IFS=\":\" read user pw uid gid name home shell &2 exit 1 fi file_info=$(grep \"^$user_name:\" $FILE) 将 grep 命令的结果复制给 file_info，grep 命令使用的正则表达式保证了用户名只会与 /etc/passwd 文件中的一条记录相匹配 IFS=\":\" read user pw uid gid name home shell 这条语句由三部分组成，一条变量赋值语句，一条带有变量名作参数的 read 命令和一个重定向运算符 shell 允许在命令执行之前对一到多个变量进行赋值，这些赋值操作会改变接下来所执行命令的操作环境 操作符 或许你会疑问下面方法是不是也可以呢？ echo \"$file_info\" | IFS=\":\" read user pw uid gid name home shell 通常 read 命令会从标准输入中获取输入，而不能采用 echo \"foo\" | read，看似这个命令应该能执行成功，但是 REPLY 变量总是空值。造成这个现象的原因主要是由于 shell 处理管道的方式。在 bash 中，管道会创造子 shell，子 shell 复制了 shell 及 pipeline 执行命令过程中使用到的 shell 环境。在上面的命令中，read 命令就是在子 shell 中执行的。类 UNIX 系统的子 shell 会执行进程复制所需的 shell 环境。进程结束后，所复制的 shell 环境即被销毁，这意味着子 shell 永远不会改变父类进程的 shell 环境。read 命令给变量赋值，这些变量会成为 shell 环境的一部分。而在上面命令中，read 将变量 foo 的值赋值给子 shell 环境中的 REPLY 变量，但是当命令退出时，子 shell 与其环境就会被销毁，read 命令的赋值效果也就丢失了。而解决此问题的方法之一就是使用嵌入字符串 第二十九章 流控制：while 和 until 循环 while 下面脚本通过 while 按顺序显示 1 ~ 5 这几个数字 #!/bin/bash # while-count: display a series of numbers count=1 while [ $count -le 5 ]; do echo $count count=$(( count + 1)) done echo \"finished.\" 脚本的执行结果如下所示 while 命令的语法结构如下 while command; do commands; done bash 同样提供了两种可用于控制循环内部程序流的内建命令—— break 和 end until C 语言里面有 while 和 do...while，until 和 while 的区别是类似的 #!/bin/bash # while-count: display a series of numbers count=1 until [ $count -gt 5 ]; do echo $count count=$(( count + 1)) done echo \"finished.\" 第三十章 故障诊断 bash 提供了一种追踪的方法，即直接使用 -x 选项或 set 命令加 -x 选项，例如 #!/bin/bash -x # trouble: script to demonstarte common errors number=1 if [ $number = 1 ]; then echo \"number is equal to 1.\" else echo \"number is not equal to 1.\" fi 其执行的结果如下所示 行开端的加号表示此行是系统的追踪信息，加号是追踪信息的默认特征，是由 shell 变量 PS4 设定的，用户可以修改变量值使追踪活动的提示符提供更多帮助信息。例如，我们使提示符包含执行追踪活动的脚本行号 如果要对脚本选定的一部分而不是整个脚本执行追踪，可以使用 set 命令加 -x 选项 第三十一章 流控制：case 分支 case 命令和 C 语言的 switch 一样，用于多项选择，其用法如下 同路径名扩展一样，case 使用以 \")\" 字符结尾的模式，下表列出了一些有效的模式 举个例子， #!/bin/bash read -p \"enter word > \" case $REPLY in [[:alpha:]]) echo \"is a single alphabetic character.\" ;; [ABC][0-9]) echo \"is A, B, or C followed by a digit.\" ;; ???) echo \"is there characters long.\" ;; *.txt) echo \"is a word ending in '.txt'\" ;; *) echo \"is something else.\" ;; esac 我们也可以使用竖线作为分隔符来组合多个模式，模式之间是“或”的条件关系，例如同时相应大小写字母的事件 第三十二章 位置参数 shell 提供了一组名为位置参数的变量，用于存户命令行中的关键字，这些变量分别命名为 0 ~ 9 #!/bin/bash # posit-param: script to view command line parameters echo \" \\$0 = $0 \\$1 = $1 \\$2 = $2 \\$3 = $3 \\$4 = $4 \\$5 = $5 \\$6 = $6 \\$7 = $7 \\$8 = $8 \\$9 = $9 这个脚本展示了从变量 $0 到 $9 的值，在没有任何命令行实参的情形下执行此脚本结果如下所示 shell 提供了变量 $#以给出命令行参数的数目 #!/bin/bash # posit-param: script to view command line parameters echo \" number of arguments: $# \\$0 = $0 \\$1 = $1 \\$2 = $2 \\$3 = $3 \\$4 = $4 \\$5 = $5 \\$6 = $6 \\$7 = $7 \\$8 = $8 \\$9 = $9 下面是脚本运行的情况 shell 提供了一种略显笨拙的方法来处理大量额度实参，每次执行 shift 命令后，所有参数的值均下移一位。实际上，通过 shift 命令我们就可以只处理一个参数（$0 之外的一个参数，$0 值固定）而完成全部程序任务。例如，下面脚本每当执行一次 shift 命令时，变量 $2 的值就赋给 $1，而 $3 的值则赋给 $2，以此类推。变量 $# 的值同减一 #!/bin/bash # posit-param2: script to display all arguments count=1 while [[ $# -gt 0 ]]; do echo \"argument $count = $1\" count=$((count+1)) shift done 上面脚本的运行情况如下 下面是一个简单的文件信息程序范例，这个程序输出了单个特定文件的文件类型（来自 file 命令）和文件状态（来自 stat 命令），程序中 PROGNAME 的值来自 basename $0 命令的执行结果。basename 命令的作用是移除路径名的起始部分，只留下基本的文件名 #!/bin/bash # file_infos: simple file information program PROGNAME=$(basename $0) if [[ -e $1 ]]; then echo -e \"\\nFile Type: \" file $1 echo -e \"\\nFile Status: \" stat $1 else echo \"$PROGNAME: usage $POGNAME file\" >&2 exit 1 fi 脚本执行的情况如下所示 处理多个位置参数 有时我们将所有的文字参数作为一个整体来处理会比较方便，shell 为这项功能提供了两种特殊的参数，如下所示 这个解释读起来有点费力，看下面脚本的执行结果就很容易理解它们之间的区别了。下面脚本中我们创建了两个实参—— words 和 words with space，并将其传递给 pass_params，pass_params 函数使用了上述的 4 种 $* 和 $@ 的方法，然后将这两个实参分别依次传递给 print_params 函数 #!/bin/bash #posit-param3: script to demonstrate $* and $@ print_params () { echo \"\\$1 = $1\" echo \"\\$2 = $2\" echo \"\\$3 = $3\" echo \"\\$4 = $4\" } pass_params () { echo -e \"\\n\" '$*: '; print_params $* echo -e \"\\n\" '\"$*\": '; print_params \"$*\" echo -e \"\\n\" '$@: '; print_params $@ echo -e \"\\n\" '\"$@\": '; print_params \"$@\" } pass_params \"words\" \"words with spaces\" 上面脚本的执行结果如下所示，可见 \"$@\" 保持了每个位置参数的完整性 第三十三章 流控制：for 循环 for：传统 shell 形式 原始的 for 命令语法如下 for variable [in words]; do commands done 例如 for：C 语言形式 新的 bash 版本加入了第二种 for 命令语法，其类似于 C 语言 for (( expression1; expression2; expression3 )); do commands done 例如 第三十四章 字符串和数字 参数扩展 参数扩展的最简单形式体现在平常对变量的使用中。举例来说，$a 扩展后成为变量 a 所包含的内容。简单参数也可以被括号包围，例如 ${a}，这对扩展本身没有什么影响。但是，当变量相邻于其他文本时，则必须使用括号，否则可能会让 shell 混淆 有的参数扩展用于处理不存在的变量和空变量。这些参数扩展在处理缺失的位置参数和给参数赋默认值时很有用处。这种参数扩展形式如下 ${parameter:-word} 如果 parameter 未被设定或者是空参数，则其扩展为 word 的值；如果 parameter 非空，则扩展为 parameter 的值 另一种扩展形式，在里面使用等号，而非连字符，用法还是相同的 ${parameterA:=word} 如果使用一个问号，若 parameter 未设定或为空，这样扩展会导致脚本出错而退出，并且 word 内容输出到标准错误 ${parameterA:?word} 如果使用一个加号，若 parameter 未设定或为空，将不产生任何扩展 shell 具有返回变量名的功能，这种功能在相当特殊的情况下才会被用到。该扩展返回当前以 prefix 开头的变量名，根据 bash 文档，这两种扩展形式执行效果一模一样 ${!prefix*} ${!prefix@} 下面的例子中，我们列出了环境变量中所有以 BASH 开头的变量 对字符串的操作，存在着大量的扩展集合。例如，下面的扩展式扩展为 parameter 内包含的字符串的长度，如果参数 parameter 是 @ 或者 *，那么扩展结果是位置参数的个数 ${#parameter} 下面的扩展用来提取一部分包含在参数 parameter 中的字符串，扩展以 offset 字符开始，知道字符串末尾或指定长度结束 ${parameter:offset} #{parameter:offset:length} 如果 offset 的值为负，表示它熊字符串末尾开始，注意，在这里负号前面必须有一个空格以避免和 ${parameter:-word} 混淆 下面扩展式根据 pattern 定义，去除包含在 parameter 中的字符串的主要部分。# 去除最短匹配，##去除最长匹配 ${parameter#pattern} ${parameter##pattern} 如果将上面的 # 换成 % 则表示从参数包含的字符串末尾去除文本，如 这个扩展在 parameter 的内容上执行搜索和替换，如果文本被发现和 pattern 一致，就被替换为 string 的内容。通常形式下，只有第一个出现的 pattern 被替换。在 // 形式下，所有 pattern 都被替换，/# 形式要求匹配出现在字符串开头，/% 要求匹配出现在字符串末尾。/string 可以省略，不过和 pattern 匹配的文本都会被删除 ${parameter/pattern/string} ${parameter//pattern/string} ${parameter/#pattern/string} ${parameter/%pattern/string} 算术计算和扩展 在算数表达式中，shell 支持任何进制表示的整数。下表列出了基本数字进制的描述 一般来说，在 C 语言中使用的算是表达式几乎都可以在 shell $((expression)) 中使用 bc：一种任意精度计算语言 如果希望实现一些更高级的数学运算，或者浮点数运算，直接使用 shell 是无法实现的。为了达到这个目的，一种方法是使用专门的计算器程序 bc bc 程序读取一个使用类 C 语言的程序文件，并执行它。例如，我们下面编写一个简单的加法脚本，bc 使用和 C 语言相同的注释方法 /* a very simple bc script */ 2 + 2 将上面脚本保存为 foo.bc 即可按照如下的方式运行 bc 也可以交互地使用 通过标准输入传递一个脚本到 bc 也是可行的 既然支持标准输入，那么意味着可以使用嵌入文档，嵌入字符串和管道传递脚本。下面是一个嵌入字符串的例子 下面我们构建一个脚本计算按月偿还贷款，使用嵌入文档传递脚本到 bc #!/bin/bash # loan-calc: script to calculate monthly loan payments PROGNAME=$(basename $0) usage () { cat 运行如下 第三十五章 数组 命令数组变量同命名其他 bash 变量一样，当访问数字变量时 可以自动创建他们 使用 declare 命令也可以创建数组，如下所示 数组的赋值有下面两种方法 数组的初始化可以通过 for 循环加花括号扩展轻松实现 我们可以使用下标 * 和 @ 来访问数组中的每个元素，可以看到如果对 ${animals[*]} 和 ${animal[@]} 加以引用就会得到不同的结果，符号 * 将数组所有内容放在一个字中，而符号 @ 使用 3 个字来显示数组的真实内容 最后两种在上面的区别不是特别明显，可以参考下面的显示结果 我们可以采用类似获取字符串长度的方式来确定数组中元素的数目，值的一提的是，当将字符串 foo 赋给元素 100 时，bash 报告数组中只有一个元素，这与其他一些编程语言的行为是不同的。在这些编程语言中，数组中未使用的元素初始化为空值，但参与计数 我们可以通过使用下标 * 和 @ 来访问数组中的每个元素，也可以通过使用下面连个参数扩展来获得元素对应的下标 ${!array[*]} ${!array[@]} 如果想在数组末尾添加一些元素，可以使用 +=，如下所示 shell 虽然没有直接的方式来完成排序功能，但是用一些代码还是可以的 #!/bin/bash # array-sort: Sort an array a=(f e d c b a) echo \"original array: ${a[@]}\" a_sorted=($(for i in \"${a[@]}\"; do echo $i; done | sort)) echo \"sorted array: ${a_sorted[@]}\" 运行结果如下所示 使用 unset 命令可以删除数组 我们也可以使用 unset 来删除单个数组元素 在 shell 中有趣的是，对数组元素赋一个空值并不意味着清空它 任何涉及到不含下标的数组变量的引用指的是数组中的元素 0，如下所示 第三十六章 其他命令 组命令和子 shell bash 允许将命令组合到一起使用，这有两种方式，一种是利用组命令，另一种是使用子 shell，值的注意的是，在 bash 实现组命令时，必须使用一个空格将花括号与命令分开，并且在闭合花括号 前使用分号或是换行来结束最后的命令 # 组命令 {command1; command2; [command3; ...]} # 子 shell (command1; command2; [command3; ...]) 执行重定向 那么组命令和子 shell 有什么用途呢？尽管他们有一处主要的区别，但是他们都可以用来管理重定向 使用组命令的话，可以写成一行 使用子 shell 也是一样的 使用这个技术，可以减少一些输入，但是组命令或是子 shell 真正有价值的地方在于管道的使用。当创建命令管道时，通常将多条命令的结果输出到一条流中，这很有用。组命令和子 shell 是的这一点变得简单 这里，我们将 3 个命令的输出进行的合并，并通过管道输出到 lpr 的输入以产生一个打印报告 进程替换 虽然组命令和子 shell 看起来相似，都可以用来为重定向整合流，但是，别忘了它们还有一处主要的不同。子 shell 在当前 shell 的子拷贝中执行命令，而组命令在当前 shell 里执行所有命令。这意味着子 shell 复制当前的环境变量以创建一个新的 shell 实例。当子 shell 退出时，复制的环境变量也就消失了，因此，任何对子 shell 环境的改变也同样丢失了。所以，大多数情况下，除非脚本需要子shell，否则使用组命令比子shell更快，占用内存也更少 在上面的例子中，REPLY的内容总是空的，因为read命令是在子shell中执行的，并且当子shell终止的时候，REPLY的拷贝也遭到了破坏 由于总是在子 shell 中执行管道中的命令，任何变量赋值的命令都会遇到这个问题。幸运的是，shell 提供了一种叫做进程替换的外部扩展方式来解决这个问题 # 产生标准输出的进程 (list) 为了解决上述 read 命令的问题，我们可以按照如下方式使用进程替换 进程替换允许将子 shell 的输出当做一个普通的文件，目的是为了重定向。事实上，这是一种扩展形式，我们可以查看它的真实值。在下面例子中，我们可以看到文件 /dev/fd/63 为子 shell 提供输出 进程替换通常结合带有 read 的循环使用，这里有一个读循环的实例，该循环用来处理子 shell 创建的目录列表的内容 #!/bin/bash # pro-sub: demo of process subtitution while read attr links owner group size data time filename; do cat trap 命令 trap 命令用于指定在接收到信号后将要采取的动作，常见的用途是在脚本程序被中断时完成清理工作，语法如下 trap argument signal [signal...] 例如， #!/bin/bash # trap-demo: simple signal handing demo trap \"echo 'I am ignoring you.'\" SIGINT SIGTERM for i in {1..5}; do echo \"iteration $i of 5\" sleep 5 done 当用户按下 Ctrl+C 试图结束脚本时，程序的执行情况如下 我们可以用 shell 函数来代替 augment 所指代的命令，如 #!/bin/bash # trap-demo: simple signal handing demo exit_on_signal_SIGINT () { echo \"script interrupted.\" 2>&1 exit 0 } exit_on_signal_SIGTERM () { echo \"script terminated.\" 2>&1 exit 0 } trap exit_on_signal_SIGINT SIGINT trap exit_on_signal_SIGTERM SIGTERM for i in {1..5}; do echo \"iteration $i of 5\" sleep 5 done 异步执行 wait 命令可以让父脚本暂停，直到指定的进程结束，下面两个脚本中，父脚本加载子脚本在后台运行，通过 $! 将子进程的 PID 记录下来，随后，父脚本执行带有子进程 PID 的 wait 命令，导致父脚本暂停，直到子脚本退出 #!/bin/bash #async-parent: asynchronous execution demo (parent) echo \"parent: starting...\" echo \"parent: launching child script...\" async-child & pid=$! echo \"parent: child (PID=$pid) launched.\" echo \"parent: continuing...\" sleep 2 echo \"parent: pausing to wait for child to finish...\" wait $pid echo \"parent: child is finished. Continuing...\" echo \"parent: parent is done. Exiting.\" #!/bin/bash #asynv-child: asynchronous execution demo (child) echo \"child: child is running...\" sleep 5 echo \"child: child is done. Exiting.\" 当执行的时候，父脚本和子脚本产生如下的输出 命令管道 大多数类 UNIX 系统支持创建一个叫做命令管道的特殊类型的文件，使用命令管道可以建立两个进程之间的通信，并且可以像其他类型的文件一样使用。命令管道的工作方式与文件相同，但实际上是两块先进先出（FIFO）的缓冲区，与普通的管道一样，数据从一端进入，从另一端出来 命名管道的使用可以以如下方式设置 process1 > named_pipe process2 执行效果等同于 process1 | process2 首先，我们创建一个命名管道，使用 mkfifo 接着，我们打开两个终端窗口。在第一个终端中，输入一条简单的命令，并将其输出重新定位到这个管道名 按下 enter，之后这个命令看起来像是挂起来了。这是因为从管道的另一端还没有接收数据，也就是常说的管道阻塞。于是，我们在第二个终端中输入如下命令 第一个终端产生的目录列表作为 cat 命令的输出出现在第二个终端窗口，而一旦管道不处于阻塞状态，第一个终端中的 ls 命令就完成了 "},"【开发手册】windows 相关.html":{"url":"【开发手册】windows 相关.html","title":"Part One. windows 相关","keywords":"","body":"设置终端代理 //使用http代理 set http_proxy=http://127.0.0.1:1081 set http_proxys=http://127.0.0.1:1081 //使用sockts5代理 set http_proxy=socks5://127.0.0.1:1080 set https_proxy=socks5://127.0.0.1:1080 验证是否添加成功了代理，不能用ping命令，因为ping命令的协议不是HTTP，也不是HTTPS，而是ICMP协议。 curl -vv http://www.google.com v2ray 下 windows terminal 代理的配置看窗口的最下栏 git bash 设置终端代理 设置代理 git config --global http.proxy http://127.0.0.1:1081 git config --global https.proxy https://127.0.0.1:1081 git config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 取消代理 git config --global --unset http.proxy git config --global --unset https.proxy 查看代理 git config --global --get http.proxy git config --global --get https.proxy "}}